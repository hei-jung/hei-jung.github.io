---
title: "[ë…¼ë¬¸ë¦¬ë·°] Deep Imbalanced Regression"
categories: paper-review
tags: machine-learning python pytorch
toc: true
toc_sticky: true
---

# DIR ë…¼ë¬¸ & ì½”ë“œ ë¦¬ë·°

[ë…¼ë¬¸: Delving into Deep Imbalanced Regression](http://proceedings.mlr.press/v139/yang21m.html), [Github](https://github.com/YyzHarry/imbalanced-regression), [ì €ìì˜ í¬ìŠ¤íŠ¸](https://towardsdatascience.com/strategies-and-tactics-for-regression-on-imbalanced-data-61eeb0921fca)

## My Preview

### Imbalanced Regression

ë‚´ê°€ í•™ìŠµì— ì‚¬ìš©í•˜ê³  ìˆëŠ” ë°ì´í„°ëŠ” Medical dataì´ë‹¤. Medical/Clinical/Healthcare dataëŠ” ë‹¤ë¥¸ ë°ì´í„°ì— ë¹„í•´ì„œ íŠ¹íˆ ë” ë¶ˆê· í˜•(imbalanced)ì¸ ê²½ìš°ê°€ ë§ë‹¤.
ì¼ë°˜ì ìœ¼ë¡œ ì§ˆë³‘ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ê±´ê°• ì •ë³´ëŠ” ë³‘ì— ê±¸ë¦° ì‚¬ëŒë³´ë‹¤ ê±¸ë¦¬ì§€ ì•Šì€ ê±´ê°•í•œ ì‚¬ëŒì˜ ë¹„ìœ¨ì´ í›¨ì”¬ ë†’ê¸° ë•Œë¬¸ì´ë‹¤.<br>
ì˜ˆë¥¼ ë“¤ì–´ ë³‘ë³€ ë°ì´í„°ë¼ê³  í•˜ë©´ ë¹„êµì  ë³‘ë³€ ìˆ˜ì¹˜ê°€ ì‘ì€ ìª½ìœ¼ë¡œ ì¹˜ìš°ì¹œ ë¶„í¬ í˜•íƒœì¸ right-skewed (positively skewed) distributionì„ ë³´ì´ê²Œ ëœë‹¤.

ì…ë ¥ ë°ì´í„°ê°€ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ëŠ” classification taskì—ì„œì˜ imbalanced data handlingì— ëŒ€í•œ ë°©ë²•ìœ¼ë¡œëŠ”
[SMOTE](https://www.jair.org/index.php/jair/article/view/10302)(<u>S</u>ynthetic <u>M</u>inority <u>O</u>versampling <u>Te</u>chnique) ê°™ì€ ê²ƒë“¤ì´ ì•Œë ¤ì ¸ ìˆë‹¤.<br>
ê·¸ëŸ°ë° ë‚´ ì—°êµ¬ ì£¼ì œëŠ” ì…ë ¥ ë°ì´í„°ë¡œ ì´ë¯¸ì§€ë¥¼ ì£¼ê³  ì‹¤ìˆ˜ê°’ì„ ì˜ˆì¸¡í•˜ê²Œ í•˜ëŠ” *regression task*ë¼ì„œ SMOTE ê°™ì€ ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸°ê°€ ì• ë§¤í•˜ë‹¤.

**Delving into Deep Imbalanced Regression** ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ° ë‚˜ì—ê²Œ í•„ìš”í•œ ë°ì´í„° ë¶ˆê· í˜• í•´ê²° ë°©ë²•ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.

## 0. Abstract

ê¸°ì¡´ì˜ ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ë“¤ì€ ë²”ì£¼í˜• ë°ì´í„° (target with categorical indices)ë“¤ì˜ ë¶ˆê· í˜• ë¬¸ì œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤. (ì£¼ë¡œ classification taskì—ì„œì˜ ë°ì´í„° ë¶ˆê· í˜•)
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì—°ì†ì ì¸ ê°’ë“¤ë¡œ êµ¬ì„±ëœ ë°ì´í„°ë¥¼ í•™ìŠµí•  ë•Œì˜ ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œ (Deep Imbalanced Regression; DIR)ë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.
ë²”ì£¼í˜• ë°ì´í„°ì™€ ì—°ì†í˜• ë°ì´í„°ì˜ ë³¸ì§ˆì ì¸ ì°¨ì´ì ì—ì„œ ì°©ì•ˆí•˜ì—¬ labelê³¼ featureì— ëŒ€í•œ distribution smoothing ê¸°ë²•ì„ ì œì•ˆí•˜ê³  ìˆê³ , ì´ëŠ” imbalanced <u>regression problem</u>ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë‹¤.

## 1. Introduction

ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œëŠ” í¸ì¬í•œë‹¤. ì´ í˜„ìƒì€ ê¸°ê³„ í•™ìŠµì—ë„ í° ë°©í•´ìš”ì†Œê°€ ë˜ê¸° ë•Œë¬¸ì— ì´ë¯¸ ì˜ˆì „ë¶€í„° ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ë“¤ì´ ë§ì´ ì œì•ˆë˜ì—ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ì†”ë£¨ì…˜ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ì‚°ì (discrete)ìœ¼ë¡œ í´ë˜ìŠ¤(class)ê°€ ë‚˜ë‰˜ì–´ ìˆëŠ”, ì´ë¥¸ë°” classification ë¬¸ì œì—ë§Œ ì£¼ë ¥í•˜ê³  ìˆë‹¤.
ì •ì‘ í˜„ì‹¤ ì„¸ê³„ì˜ ë°ì´í„°ëŠ” ì—°ì†ì ì´ê³  ë²”ìœ„ê°€ ë¬´í•œí•œ ê°’ë“¤ë¡œ ì´ë¤„ì ¸ ìˆìœ¼ë©° ì´ëŸ¬í•œ ì—°ì†í˜• ë°ì´í„° ì•ˆì—ì„œë„ ë¶ˆê· í˜• ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°ê°€ ë§ë‹¤.
ê·¸ ëŒ€í‘œì ì¸ ì˜ˆì‹œê°€ ë‚´ê°€ ê°€ì§€ê³  ìˆëŠ” ê²ƒê³¼ ê°™ì€ medical/clinical dataì´ë‹¤.

imbalanced regressionì€ imbalanced classificationì´ ê°€ì§€ëŠ” ë¬¸ì œì ê³¼ëŠ” ë‹¤ë¥¸ ë¬¸ì œì ë“¤ì´ ìˆëŠ”ë°, ì—¬ê¸°ì„  ëŒ€í‘œì ìœ¼ë¡œ 3ê°€ì§€ë¥¼ ë“¤ê³  ìˆë‹¤.

ë¨¼ì € ì—°ì†ì ì¸ íƒ€ê¹ƒ ê°’ë“¤ì€ (ì•ìœ¼ë¡œ íƒ€ê¹ƒ(target)ì´ë¼ê³  í•˜ë©´ ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ì— í•™ìŠµì‹œí‚¬ ëŒ€ìƒì„ ì§€ì¹­í•œë‹¤.) í´ë˜ìŠ¤ ê°„ ëšœë ·í•œ ê²½ê³„ê°€ ì—†ê¸° ë•Œë¬¸ì— re-samplingì´ë‚˜ re-weighting ê°™ì€ ì „í†µì ì¸ imbalanced classification ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸°ì—” ì• ë§¤í•´ì§„ë‹¤.<br>
ì´ê²Œ ë¬´ìŠ¨ ë§ì´ëƒ í•˜ë©´, ì‹¤ìˆ˜ê°’ ë°ì´í„°ëŠ” classificationìœ¼ë¡œ ì¹˜ë©´ ê·¸ ê°’ ìì²´ë¡œ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ê°€ ë˜ì–´ë²„ë¦°ë‹¤.
SMOTE ê°™ì€ re-sampling ê¸°ë²•ì€ ë³´í†µ ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œ ì‚¬ì´ì‚¬ì´ì— ì„ì˜ì˜ ê°’ì„ ìƒˆë¡œ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë¤„ì§€ëŠ”ë°, ì—°ì† ë°ì´í„°ëŠ” ê·¸ ê°’ ìì²´ë¡œ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë‹ˆê¹Œ í´ë˜ìŠ¤ 'ì‚¬ì´ì‚¬ì´' ê³µê°„ì„ ì •ì˜í•˜ê¸° ë§¤ìš° ì–´ë ¤ì›Œì§€ëŠ” ê²ƒì´ë‹¤.

ë°©ê¸ˆ ì „ ì–˜ê¸°ì™€ ê°™ì€ ë§¥ë½ì—ì„œ ì—°ì†ì ì¸ íƒ€ê¹ƒ ë°ì´í„°ë“¤ì€ íƒ€ê¹ƒ ê°’ë“¤ ê°„ ê±°ë¦¬ì—ë„ ì˜ë¯¸ê°€ ìˆìœ¼ë©°, ì´ê±´ ë°ì´í„° ë¶ˆê· í˜•ì„ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í• ì§€ì— ëŒ€í•œ íŒíŠ¸ë¥¼ ì¤€ë‹¤.<br>
ì˜ˆë¥¼ ë“¤ì–´ì„œ training ë°ì´í„°ì…‹ì—ì„œ t1, t2ë¼ëŠ” ë‘ ê°œì˜ íƒ€ê¹ƒì´ ìˆë‹¤ê³  í•´ë³´ì.<br>
![Challenge 2](/assets/images/dir_challenge_2.png)<br>
<span style="font-size:xx-small">(ì´ë¯¸ì§€ ì¶œì²˜: [ì €ìì˜ í¬ìŠ¤íŠ¸](https://towardsdatascience.com/strategies-and-tactics-for-regression-on-imbalanced-data-61eeb0921fca))</span><br>
ì—¬ê¸°ì„œ t1 ì£¼ë³€ì˜ ê°’ë“¤ì€ ë¹ˆë„ê°€ ë†’ì€ ë°˜ë©´ ([t1-âˆ†, t1+âˆ†] ë²”ìœ„ ì•ˆì— ìƒ˜í”Œ ìˆ˜ê°€ ë§ìŒ) t2ëŠ” ê·¸ë ‡ì§€ê°€ ì•Šì€ ê²½ìš°,
ë¹„ë¡ t1ì˜ ê°œìˆ˜ì™€ t2ì˜ ê°œìˆ˜ê°€ ê°™ì„ì§€ë¼ë„ ë‘ íƒ€ê¹ƒì˜ ë¶ˆê· í˜• ì •ë„ê°€ ì„œë¡œ ê°™ë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ classificationê³¼ ë‹¬ë¦¬, ì–´ë–¤ íƒ€ê¹ƒ ê°’ë“¤ì€ ë°ì´í„°ê°€ ì•„ì˜ˆ ì¡´ì¬í•˜ì§ˆ ì•Šì•„ì„œ íƒ€ê¹ƒì˜ extrapolationì´ë‚˜ interpolationì„ í•„ìš”ë¡œ í•œë‹¤.<br>
![Challenge 3](/assets/images/dir_challenge_3.png)<br>
<span style="font-size:xx-small">(ì´ë¯¸ì§€ ì¶œì²˜: ì €ìì˜ í¬ìŠ¤íŠ¸)</span>

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ DIRì„ ë‹¤ë£¨ëŠ” ë°©ë²• ë‘ ê°€ì§€ë¥¼ ì„ ë³´ì¸ë‹¤: **label distribution smoothing (LDS)** and **feature distribution smoothing (FDS)**.
ë‘ ì ‘ê·¼ë²•ì— ëŒ€í•œ í•µì‹¬ì ì¸ ë°œìƒì€ kernel distributionì„ í™œìš©í•´ì„œ ì¸ì ‘í•œ íƒ€ê¹ƒë“¤ ê°„ì˜ ìœ ì‚¬ë„ (similarity)ë¥¼ ì´ìš©í•¨ìœ¼ë¡œì¨ label spaceì™€ feature space ìƒì—ì„œ distribution smoothingì„ í•´ì£¼ëŠ” ê²ƒì´ë‹¤.
ë‘ ê¸°ìˆ  ëª¨ë‘ ê¸°ì¡´ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì— ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆê³  end-to-end ë°©ì‹ì˜ optimizationì´ ê°€ëŠ¥í•˜ë‹¤.
ë˜í•œ ì´ ê¸°ìˆ ë“¤ì€ ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ í•¨ê»˜ ì‚¬ìš©í–ˆì„ ë•Œ ë”ìš±ë” íš¨ê³¼ì ì´ë‹¤.

## 2. Related work

ëŒ€ì¶© ê¸°ì¡´ì˜ **Imbalanced Classification** (SMOTE ë“±ë“±...), **Imbalanced Regression** (SMOTE í™œìš©...) ê³¼ëŠ” ì°¨ë³„í™” ë˜ì—ˆë‹¤ëŠ” ì´ì•¼ê¸°.

## 3. Methods

<!--
### Problem settings

`{(xi, yi)}N,i=1`ì´ë¼ëŠ” training setì´ ìˆë‹¤ê³  í•˜ì. (`xi`: ì‹¤ìˆ˜ input, `yi`: ì‹¤ìˆ˜ label)
ì—¬ê¸°ì„œ label space `Y`ì™€ íƒ€ê¹ƒ ê°’ì˜ ê·¸ë£¹ ì¸ë±ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” `B` ì§‘í•©ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.
-->

### 3.1. Label Distribution Smoothing

LDS ì„¤ëª…ì— ì•ì„œ classificationê³¼ regressionì˜ ì°¨ì´ì ì— ëŒ€í•´ì„œ ì˜ˆì‹œë¥¼ ê°€ì§€ê³  ì„¤ëª…í•œë‹¤.

> Motivating Example.

label ë²”ìœ„ê°€ 0~99ë¡œ ê°™ê³ , label density distributionì´ ê°™ì€ `(1) 100ê°œì§œë¦¬ í´ë˜ìŠ¤ì˜ classification example`ê³¼ `(2) large-scaleì˜ regression example`ì„ ê°€ì§€ê³  ì„¤ëª…í•´ë³´ê² ë‹¤.

![Figure 2](/assets/images/dir_figure_2.png)

íŒŒë€ìƒ‰ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œì‹œëœ ìœ„ìª½ ê·¸ë¦¼ì€ ê° ë°ì´í„°ì…‹ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ê³ , ë¹¨ê°„ìƒ‰ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œì‹œëœ ì•„ë˜ìª½ ê·¸ë¦¼ì€ í•™ìŠµ í›„ì˜ label ë³„ test errorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.<br>
*Figure 2.(a)*ë¥¼ ë³´ë©´ error distributionê³¼ label density distributionì— correlationì´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ê·¸ëŸ°ë° ì—°ì† ë°ì´í„°ì…‹ì— ëŒ€í•œ ê·¸ë˜í”„ì¸ *Figure 2.(b)*ë¥¼ ë³´ë©´ *Figure 2.(a)*ì— ë¹„í•´ì„œ error distributionì´ í›¨ì”¬ smoothí•˜ë©° label density distributionê³¼ correlationì´ ê±°ì˜ ì—†ë‹¤ëŠ” ê²ƒì´ í™•ì¸ëœë‹¤.

ì—¬ê¸°ì„œ ëˆˆì—¬ê²¨ë³¼ ì ì€ ëª¨ë“  imbalanced learning methodëŠ” ì§ê°„ì ‘ì ìœ¼ë¡œ *ì‹¤ì¦ì ì¸ (empirical)* label density distributionì˜ ë¶ˆê· í˜•ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ì‹ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.
ì´ëŸ¬í•œ ë°©ì‹ì€ í´ë˜ìŠ¤ label ë¶ˆê· í˜•ì— ëŒ€í•´ì„œëŠ” ì ì ˆí•˜ì§€ë§Œ ì—°ì†ì ì¸ label ê°’ë“¤ì— ëŒ€í•´ì„  í•™ìŠµ ëª¨ë¸ì—ì„œ ë³´ì´ëŠ” ê²ƒë§Œí¼ì˜ ë¶ˆê· í˜•ì„ ê·¸ëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤.
ì´ëŸ° ì´ìœ ë¡œ ì—°ì†ì ì¸ label spaceì—ì„œ ì´ ë°©ì‹ì„ ì ìš©í•˜ëŠ” ê²ƒì€ ì ì ˆí•˜ì§€ ì•Šë‹¤.

> LDS for Imbalanced Data Density Estimation.

ì•ì„  ì˜ˆì‹œì—ì„œ ì—°ì†ì ì¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” empirical label density distributionì´ ì‹¤ì œ label density distributionì„ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ë³´ì˜€ë‹¤. ì´ê²ƒì€ ì¸ì ‘í•œ label ë°ì´í„° ìƒ˜í”Œë“¤ ê°„ì˜ ì˜ì¡´ì„±(dependence) ë•Œë¬¸ì´ë‹¤.<br>
Label Distribution Smoothing (LDS)ëŠ” ì—°ì†ì ì¸ íƒ€ê¹ƒ ë°ì´í„° ê°„ ë¶ˆê· í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ kernel densityì„ ì˜ˆì¸¡í•œë‹¤.

LDSëŠ” symmetric kernelê³¼ empirical density distributionì„ ê°€ì§€ê³  convolutionì„ í•´ì„œ ì¸ì ‘í•œ labelì˜ ë°ì´í„° ìƒ˜í”Œë¼ë¦¬ ê²¹ì¹œ kernel-smoothed ë²„ì „ì„ ë§Œë“¤ì–´ë‚¸ë‹¤.
ì—¬ê¸°ì„œ symmetric kernelì´ë¼ëŠ” ê±´ `k(y, y')=k(y', y)`ì™€ `âˆ‡y(k(y, y'))+âˆ‡y(k(y', y))=0, âˆ€y,y'âˆˆY`ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  kernelì„ ì˜ë¯¸í•œë‹¤.
(ì´ ë…¼ë¬¸ì—ì„œ `Y`ëŠ” label spaceë¥¼ ëœ»í•¨)
ì°¸ê³ ë¡œ Gaussian kernelê³¼ Laplace kernelì€ symmetric kernelì˜ ì¼ì¢…ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆê³  `k(y, y')=yy'` ê°™ì€ ê±´ symmetricí•˜ì§€ ì•Šë‹¤.
LDSëŠ” ê²°êµ­ *effective label density distribution*ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•˜ê²Œ ëœë‹¤:

![Formula 1](/assets/images/dir_formula_1.png)

`p(y)`ëŠ” training dataì—ì„œì˜ label ê°œìˆ˜ê³ , `p~(y')`ëŠ” y' labelì˜ effective densityì´ë‹¤.

![Figure 3](/assets/images/dir_figure_3.png)

*Figure 3*ì€ LDSì™€ LDSê°€ label density distributionì„ ì–´ë–»ê²Œ smoothí•˜ê²Œ ë§Œë“œëŠ”ì§€ë¥¼ ë³´ì¸ë‹¤.

```python
from scipy.ndimage import gaussian_filter1d
from scipy.signal.windows import triang
...

def get_lds_kernel_window(kernel, ks, sigma):
    assert kernel in ['gaussian', 'triang', 'laplace']
    half_ks = (ks - 1) // 2
    if kernel == 'gaussian':
        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks
        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))
    elif kernel == 'triang':
        kernel_window = triang(ks)
    else:
        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)
        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))

    return kernel_window
```

ë…¼ë¬¸ì—ì„œ ì œê³µí•˜ëŠ” ê¹ƒí—™ ë ˆí¬ì—ì„œ ìœ„ì˜ [LDS kernelì„ ìƒì„±í•˜ëŠ” ì½”ë“œ](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/utils.py#L110)ë¥¼ ê°€ì ¸ì™”ë‹¤.<br>
ì½”ë“œë¥¼ ë³´ë©´ ìœ„ì—ì„œ symmetric kernelì´ë¼ê³  ì–¸ê¸‰í•œ gaussian í•„í„°ì™€ laplace í•„í„°ë¥¼ êµ¬í˜„í•˜ê³  ìˆë‹¤. triangì€ triangle window, ì¦‰ ì‚¼ê°í˜• í•„í„°ì´ë‹¤.
gaussian í•„í„°ì™€ ì‚¼ê°í˜• í•„í„°ëŠ” ì„¤ëª…í•˜ì§€ ì•Šì•„ë„ ê·¸ë¦¼ì„ ë³´ë©´ ê·¸ëƒ¥ ë´ë„ ëª¨ë‘ symmetricí•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤:

![Gaussian](/assets/images/Gaussian_Filter.png)<br>
[ì¶œì²˜: Wikipedia](https://en.wikipedia.org/wiki/Gaussian_filter)

![Triangle](/assets/images/scipy-signal-windows-triang.png)<br>
[ì¶œì²˜: Scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.triang.html)

laplace í•„í„°ëŠ” discrete domainì—ì„œ ì“°ì´ëŠ” í•„í„°ë¡œ, ëŒ€ì¶© ì´ëŸ° í˜•íƒœë¥¼ ìƒê°í•˜ë©´ ëœë‹¤:

![Laplace](/assets/images/laplace_filter.png)
[ì¶œì²˜: Wikipedia](https://en.wikipedia.org/wiki/Discrete_Laplace_operator)

ì¦‰ í•œê°€ìš´ë°ì˜ ì›ì†Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­ì¸ matrixì´ë‹¤.

<span style="color:blue">
!ê·¸ëƒ¥ ë‚´ ìƒê°!<br>
scipy.signal.windowsì—ì„œëŠ” ì‚¼ê°í˜• ë§ê³ ë„ ë‹¤ì–‘í•œ ëª¨ì–‘ì˜ symmetricí•œ í•„í„°ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤.<br>
ì»¤ìŠ¤í„°ë§ˆì´ì§•ì„ ìœ„í•´ ë‹¤ë¥¸ í•„í„°ë“¤ë„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆì„ê¹Œ?<br>
</span>

LDSë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìœ¼ë©°,

```python
from collections import Counter
from scipy.ndimage import convolve1d
from utils import get_lds_kernel_window

# preds, labels: [Ns,], "Ns" is the number of total samples
preds, labels = ..., ...
# assign each label to its corresponding bin (start from 0)
# with your defined get_bin_idx(), return bin_index_per_label: [Ns,]
bin_index_per_label = [get_bin_idx(label) for label in labels]

# calculate empirical (original) label distribution: [Nb,]
# "Nb" is the number of bins
Nb = max(bin_index_per_label) + 1
num_samples_of_bins = dict(Counter(bin_index_per_label))
emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(Nb)]

# lds_kernel_window: [ks,], here for example, we use gaussian, ks=5, sigma=2
lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=5, sigma=2)
# calculate effective label distribution: [Nb,]
eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')
```

(bin indexëŠ” ë°ì´í„° ê°’ë“¤ì˜ ë²”ìœ„ì— êµ¬ê°„(bin)ì„ ë‚˜ëˆ ì„œ êµ¬ê°„ë§ˆë‹¤ í• ë‹¹í•´ì£¼ëŠ” idë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.)

ìœ„ì—ì„œ êµ¬í•œ effective label distribution ì¶”ì •ì¹˜ë¥¼ ê°€ì§€ê³  loss re-weightingì„ í•  ìˆ˜ ìˆë‹¤.
loss functionì— ê° íƒ€ê¹ƒì— LDSë¥¼ ì·¨í•œ label density ì˜ˆì¸¡ì¹˜ì˜ ì—­ìˆ˜ë¥¼ ê³±í•´ì„œ re-weightingì„ í•¨ìœ¼ë¡œì¨ cost-sensitiveí•˜ê²Œ [loss functionì„ re-weighting](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/loss.py#L5) í•˜ëŠ” ê²ƒì´ë‹¤.<br>
ê·¸ê²ƒì„ ì½”ë“œë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.

```python
from loss import weighted_mse_loss

# Use re-weighting based on effective label distribution, sample-wise weights: [Ns,]
eff_num_per_label = [eff_label_dist[bin_idx] for bin_idx in bin_index_per_label]
weights = [np.float32(1 / x) for x in eff_num_per_label]

# calculate loss
loss = weighted_mse_loss(preds, labels, weights=weights)
```

```python
def weighted_mse_loss(inputs, targets, weights=None):
    loss = (inputs - targets) ** 2
    if weights is not None:
        loss *= weights.expand_as(loss)
    loss = torch.mean(loss)
    return loss
```

ì½”ë“œë¥¼ í†µí•´ LDSëŠ” í•™ìŠµ ì¤‘ ì˜ˆì¸¡ê°’ì˜ ë¶„ì‚° ì •ë„ë¥¼ ê°€ì§€ê³  lossë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” í˜•íƒœë¡œ ì´í•´í•˜ì˜€ë‹¤.

### 3.2. Feature Distribution Smoothing

ì•ì„  ì˜ˆì‹œë“¤ì„ í†µí•´ ì´ì œ target spaceì—ì„œì˜ ì—°ì†ì„±ì€ feature spaceì—ì„œì˜ ì—°ì†ì„±ì— ëŒ€ì‘í•´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.
ì¦‰ í•™ìŠµ ëª¨ë¸ì´ ì œëŒ€ë¡œ ëŒì•„ê°€ê³  ë°ì´í„° ë¶„í¬ê°€ ê· í˜• ì¡í˜€ ìˆë‹¤ë©´, ì¸ì ‘ íƒ€ê¹ƒì— ëŒ€í•œ feature í†µê³„ë¥¼ ë´¤ì„ ë•Œ ì¸ì ‘í•œ ì• ë“¤ë¼ë¦¬ëŠ” ì„œë¡œ ë¹„ìŠ·í•œ feature ê°’ì„ ë±‰ì–´ë‚¼ ê²ƒì´ë‹¤.

> Motivating Example.

ì˜ˆì‹œë¡œ í•™ìŠµëœ featureì˜ í†µê³„ë¥¼ ë¶„ì„í•´ë³´ë ¤ê³  í•œë‹¤.
ê° binì— ì†í•œ íƒ€ê¹ƒ ë°ì´í„°ì— ëŒ€í•œ í‰ê· (`ğœ‡`)ê³¼ í‘œì¤€í¸ì°¨(`ğœ`)ë¥¼ ê³„ì‚°í•œ ê²ƒì„ ì´ ë…¼ë¬¸ì—ì„œëŠ” ![feature bin denotement](/assets/images/dir_feature_bin.png){: width="100" height="100"}ë¡œ í‘œê¸°í•œë‹¤. (ì—¬ê¸°ì„œ `b`ëŠ” íƒ€ê¹ƒ ê°’ì˜ group index ë˜ëŠ” bin indexë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.)<br>
ê·¸ëŸ¬ê³  ë‚˜ì„œ feature í†µê³„ ê°„ì˜ ìœ ì‚¬ ì •ë„ë¥¼ *Figure 4*ì˜ ë‘ ê·¸ë˜í”„ì²˜ëŸ¼ ì‹œê°í™” í•  ìˆ˜ ìˆë‹¤.
*Figure 4*ëŠ” íƒ€ê¹ƒ ê°’ 30ì„ ê¸°ì¤€ìœ¼ë¡œ (`b` ê¸°ì¤€ê°’ = 30) ê° label ê°’ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ ë¶„í¬ë¥¼ ë³´ì—¬ì¤€ë‹¤.

![Figure 4](/assets/images/dir_figure_4.png)

ì•„ë˜ìª½ variance cosine similarity ê·¸ë˜í”„ë¥¼ í†µí•´ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì•Œ ìˆ˜ ìˆë‹¤. ì¦‰ í•‘í¬ìƒ‰ìœ¼ë¡œ í‘œì‹œí•œ êµ¬ê°„ì€ ë°ì´í„° ì–‘ì´ ìƒëŒ€ì ìœ¼ë¡œ ë§¤ìš° ì ë‹¤.
ìœ„ìª½ mean cosine similarity ê·¸ë˜í”„ì—ì„œ í•‘í¬ìƒ‰ êµ¬ê°„ ì¤‘ì—ì„œë„ 0~6 ë²”ìœ„ì˜ ë§‰ëŒ€ë“¤ì„ ì‚´í´ë³´ë©´, ê¸°ì¤€ê°’ì—ì„œì˜ ë§‰ëŒ€ì™€ ë†’ì´ê°€ ê±°ì˜ ë¹„ìŠ·í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ë‹¤ì‹œ ë§í•´ì„œ 0~6 ë²”ìœ„ ì•ˆì— ìˆëŠ” ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ì§€ ì•Šë‹¤ ë³´ë‹ˆ ì´ ë°ì´í„° ì¤‘ ê°€ì¥ ë§ì€ ë¶„í¬ë¥¼ ì°¨ì§€í•˜ëŠ” ëŒ€ëµ 30ì— ê°€ê¹Œìš´ ê°’ì„ ë±‰ì–´ë‚´ëŠ” ê²ƒì´ë‹¤.

> FDS Algorithm.

feature distribution smoothing (FDS)ëŠ” ë°”ë¡œ ì´ëŸ° ì ì—ì„œ ê³ ì•ˆí•œ ê²ƒì´ë‹¤.<br>
FDSëŠ” feature í†µê³„ë¥¼ ê·¼ì²˜ íƒ€ê¹ƒ ì˜ì—­ìœ¼ë¡œ ì´ë™ ì‹œí‚¨ë‹¤. ì´ë§ì¸ì¦‰ìŠ¨ ì¹˜ìš°ì¹œ ë°ì´í„°, íŠ¹íˆ <u>ê°œìˆ˜ê°€ ë¶€ì¡±í•œ ë°ì´í„°</u>ì˜ ì˜ˆì¸¡ì¹˜ë¥¼ ë³´ì •í•˜ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.
ì´ ë³´ì • ë°©ë²•ì— ëŒ€í•œ ì„¤ëª…ì€ ì•„ì§ ì´í•´í•˜ì§€ ëª»í•´ì„œ ì¶”í›„ì— ì¶”ê°€ë¡œ ì •ë¦¬í•˜ê² ë‹¤.

<!--FDS is performed by first estimating the statistics of each bin.
Without loss of generality, we substitute variance with covariance to reflect also the relationship between the various feature elements within z, where Nb is the total number of samples in `b`th bin.
Given the feature statistics, we employ again a symmetric kernel `k(yb,yb')` to smooth the distribution of the feature mean and covariance over the target bins `B`. This results in a smoothed version of the statistics.
With both `{ğœ‡b,ğ›´b}` and `{ğœ‡b~,ğ›´b~}`, we then follow the standard whitening and re-coloring procedure to calibrate the feature representation for each input sample.-->

![Figure 5](/assets/images/dir_figure_5.png)

FDSëŠ” [ë§ˆì§€ë§‰ feature mapì„ ë½‘ì•„ë‚´ëŠ” layer ë‹¤ìŒì— feature ë³´ì • layer, ì¦‰ FDS layerë¥¼ ë¼ì›Œë„£ëŠ” ë°©ì‹](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/resnet.py#L144)ìœ¼ë¡œ êµ¬í˜„í•œë‹¤.

```python
from fds import FDS

config = dict(feature_dim=..., start_update=0, start_smooth=1, kernel='gaussian', ks=5, sigma=2)

def Network(nn.Module):
    def __init__(self, **config):
        super().__init__()
        self.feature_extractor = ...
        self.regressor = nn.Linear(config['feature_dim'], 1)  # FDS operates before the final regressor
        self.FDS = FDS(**config)

    def forward(self, inputs, labels, epoch):
        features = self.feature_extractor(inputs)  # features: [batch_size, feature_dim]
        # smooth the feature distributions over the target space
        smoothed_features = features    
        if self.training and epoch >= config['start_smooth']:
            smoothed_features = self.FDS.smooth(smoothed_features, labels, epoch)
        preds = self.regressor(smoothed_features)

        return {'preds': preds, 'features': features}
```

ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë ¤ë©´ ê° epochë§ˆë‹¤ `{ğœ‡b,ğ›´b}`ì˜ [*momentum update*](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/train.py#L280)ë¥¼ í•´ì¤€ë‹¤.
ì´ ì‘ì—…ì„ í†µí•´ì„œ í•™ìŠµí•˜ëŠ” ë™ì•ˆ ì¢€ ë” ì•ˆì •ì ì´ê³  ì •í™•í•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•´ì§„ë‹¤.

```python
model = Network(**config)

for epoch in range(num_epochs):
    for (inputs, labels) in train_loader:
        # standard training pipeline
        ...

    # update FDS statistics after each training epoch
    if epoch >= config['start_update']:
        # collect features and labels for all training samples
        ...
        # training_features: [num_samples, feature_dim], training_labels: [num_samples,]
        training_features, training_labels = ..., ...
        model.FDS.update_last_epoch_stats(epoch)
        model.FDS.update_running_stats(training_features, training_labels, epoch)
```

<!--We integrate FDS into deep networks by inserting a feature calibration layer after the final feature map.
To train the model, we employ a *momentum update* of the running statistics `{ğœ‡b,ğ›´b}` across each epoch.-->

## 4. Benchmarking DIR

ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì„ ì–´ë–»ê²Œ ì‹œí—˜í–ˆëŠ”ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê³  ìˆë‹¤.<br>
*IMDB-WIKI* ë°ì´í„°ì…‹ì—ì„œ bin ê°’ì€ 0~7149 ì¤‘ì— ìˆìœ¼ë©° validation ë° test setì˜ ë°ì´í„° ë¶„í¬ê°€ ê³ ë¥´ê²Œë” ì§ì ‘ êµ¬ì„±í•˜ì˜€ë‹¤.

![Table 1](/assets/images/dir_table_1.png)

*Table 1*ì—ì„œ `Vanilla`ëŠ” imbalance handlingì„ ì•„ì˜ˆ ì•ˆ í•œ ê²ƒ,
`Focal-R`ì€ classificationì—ì„œ ì“°ì´ëŠ” `Focal loss`ë¼ëŠ” loss í•¨ìˆ˜ì˜ regression ë²„ì „,
`RRT`ëŠ” featureì™€ classifierë¥¼ ë¶„ë¦¬ì‹œì¼œì„œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê¸°ë²•ì¸ `Two-stage training`ì˜ regression ë²„ì „ì´ë‹¤.
`INV`ì™€ `SQINV`ëŠ” ê°ê° inverse-frequency weighting variant, square-root weighting variantë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

ì € í‘œë¥¼ ë³´ë©´ LDS, FDS, ê·¸ë¦¬ê³  LDSë‘ FDSë¥¼ ë‘˜ ë‹¤ ì ìš©í–ˆì„ ë•Œ ì•„ë¬´ê²ƒë„ ì ìš© ì•ˆ í–ˆì„ ë•Œë³´ë‹¤ MAEê°€ ì‘ìœ¼ë¯€ë¡œ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë” ì¢‹ì•„ì¡Œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.
