---
title: "[ë…¼ë¬¸ë¦¬ë·°] Deep Imbalanced Regression"
categories: paper-review
tags: machine-learning python pytorch
toc: true
toc_sticky: true
---

# DIR ë…¼ë¬¸ ë¦¬ë·°

[ë…¼ë¬¸: Delving into Deep Imbalanced Regression](http://proceedings.mlr.press/v139/yang21m.html), [Github](https://github.com/YyzHarry/imbalanced-regression)

## My Preview

### Imbalanced Regression

ë‚´ê°€ í•™ìŠµì— ì‚¬ìš©í•˜ê³  ìˆëŠ” ë°ì´í„°ëŠ” Medical dataì´ë‹¤. Medical/Healthcare dataëŠ” ë‹¤ë¥¸ ë°ì´í„°ì— ë¹„í•´ì„œ íŠ¹íˆ ë” ë¶ˆê· í˜•(imbalanced)ì¸ ê²½ìš°ê°€ ë§ë‹¤.
ì¼ë°˜ì ìœ¼ë¡œ ì§ˆë³‘ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ê±´ê°• ì •ë³´ëŠ” ë³‘ì— ê±¸ë¦° ì‚¬ëŒë³´ë‹¤ ê±¸ë¦¬ì§€ ì•Šì€ ê±´ê°•í•œ ì‚¬ëŒì˜ ë¹„ìœ¨ì´ í›¨ì”¬ ë§ê¸° ë•Œë¬¸ì´ë‹¤.<br>
(ë³‘ë³€ ë°ì´í„°ë¼ê³  í•˜ë©´ ë¹„êµì  ë³‘ë³€ ìˆ˜ì¹˜ê°€ ì‘ì€ ìª½ìœ¼ë¡œ ì¹˜ìš°ì¹œ ë¶„í¬ í˜•íƒœì¸ right-skewed (positively skewed) distributionì„ ë³´ì´ê²Œ ëœë‹¤.)

1D ë°ì´í„° (ì£¼ë¡œ numerical data) í•™ìŠµ ì‹œ imbalanced data handlingì— ëŒ€í•œ ë°©ë²•ìœ¼ë¡œëŠ”
[SMOTE](https://www.jair.org/index.php/jair/article/view/10302)(Synthetic Minority Oversampling TEchnique),
[SMOGN](http://proceedings.mlr.press/v74/branco17a)(SMOte for regression with Gaussian Noise) ê°™ì€ ê²ƒë“¤ì´ ì•Œë ¤ì ¸ ìˆë‹¤.
ê·¸ëŸ°ë° ë‚´ê°€ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë°ì´í„°ì˜ ê²½ìš° 3D ì´ë¯¸ì§€ ë°ì´í„°ë¼ì„œ SMOTEë‚˜ SMOGN ê°™ì€ ë°©ë²•ì„ ì ìš©í•˜ëŠ” ê±´ ì ì ˆí•˜ì§€ ì•Šë‹¤.
ë‚´ ë°ì´í„° ë¿ë§Œ ì•„ë‹ˆë¼ ìš”ì¦˜ì€ ì˜ìƒ/ë™ì˜ìƒ ë°ì´í„° í•™ìŠµì„ ë§ì´ í•˜ê¸° ë•Œë¬¸ì— ë°ì´í„° ë¶ˆê· í˜•ì„ ì†ë³´ê¸° ìœ„í•œ ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ ì°¾ì•„ë´ì•¼ í•œë‹¤.

**Delving into Deep Imbalanced Regression** ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ° ë‚˜ì—ê²Œ í•„ìš”í•œ imbalanced data handling í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.

## 0. Abstract

ê¸°ì¡´ì˜ ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ë“¤ì€ ë²”ì£¼í˜• ë°ì´í„° (target with categorical indices)ë“¤ì˜ ë¶ˆê· í˜• ë¬¸ì œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤. (ì£¼ë¡œ classification taskì—ì„œì˜ ë°ì´í„° ë¶ˆê· í˜•)
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì—°ì†ì ì¸ target valueë¡œ êµ¬ì„±ëœ ë°ì´í„°ë¡œ í•™ìŠµí•  ë•Œ ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œ (Deep Imbalanced Regression; DIR)ë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.
ë²”ì£¼í˜• labelê³¼ ì—°ì†í˜• labelì— ë‚´ì¬ì ì¸ ì°¨ì´ê°€ ìˆë‹¤ëŠ” ë°ì„œ ì°©ì•ˆí•˜ì—¬, labelê³¼ featureì— ëŒ€í•œ distribution smoothing ê¸°ë²•ì„ ì œì•ˆí•œë‹¤. ì´ëŠ” ëª…ì‹œì ìœ¼ë¡œ ê·¼ì²˜ targetì˜ íš¨ê³¼ë¥¼ ì¸ì§€í•˜ê³  ë³´ì •í•œë‹¤.

> Motivated by the intrinsic difference between categorical and continuous label space, we propose distribution smoothing for both labels and features, which explicitly acknowledges the effects of nearby targets, and calibrates both label and learned feature distributions.
We curate and benchmark large-scale DIR datasets from common real-world tasks in computer vision, natural language processing, and healthcare domains.
Extensive experiments verify the superior performance of our strategies.
Extensive experiments verify the superior performance of our strategies.
Our work fills the gap in benchmarks and techniques for practical imbalanced regression problems.

## 1. Introduction

ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œëŠ” í¸ì¬í•˜ë‹¤. ì´ í˜„ìƒì€ ê¸°ê³„ í•™ìŠµì—ë„ í° ë°©í•´ìš”ì†Œê°€ ë˜ê¸° ë•Œë¬¸ì— ì´ë¯¸ ì „ë¶€í„° ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ë“¤ì´ ë§ì´ ì œì•ˆë˜ì—ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ì†”ë£¨ì…˜ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ classê°€ ë‚˜ë‰˜ì–´ ìˆëŠ” ì´ë¥¸ë°” classification ë¬¸ì œì—ë§Œ ì£¼ë ¥í•˜ê³  ìˆë‹¤.
ì •ì‘ í˜„ì‹¤ ì„¸ê³„ì˜ ë°ì´í„°ëŠ” ì—°ì†ì ì´ê³  ë²”ìœ„ê°€ ë¬´í•œí•œ ê°’ë“¤ë¡œ ì´ë¤„ì ¸ ìˆìœ¼ë©° ì´ëŸ¬í•œ ì—°ì†ê°’ ë°ì´í„° ì•ˆì—ì„œë„ ë¶ˆê· í˜• ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°ê°€ ë§ë‹¤.

DIRì—ì„œëŠ” classification taskì— ëŒ€í•œ imbalanced data handlingê³¼ëŠ” ë‹¤ë¥¸ ìƒˆë¡œìš´ ë„ì „ì¥ì„ ë‚´ë¯¼ë‹¤. (ì—¬ê¸°ì„œ ì•ìœ¼ë¡œ íƒ€ê¹ƒ(target)ì´ë¼ê³  í•˜ë©´ ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ì— í•™ìŠµì‹œí‚¬ ëŒ€ìƒì„ ë§í•œë‹¤.)<br>
ë¨¼ì € ì—°ì†ì ì¸ íƒ€ê¹ƒ ê°’ë“¤ì€ í´ë˜ìŠ¤ ê°„ ëšœë ·í•œ ê²½ê³„ê°€ ì—†ê¸° ë•Œë¬¸ì— re-samplingì´ë‚˜ re-weighting ê°™ì€ ì „í†µì ì¸ imbalanced classification ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸°ì—” ì• ë§¤í•´ì§„ë‹¤.<br>
ê·¸ë¦¬ê³  ì—°ì†ì ì¸ labelë“¤ì€ íƒ€ê¹ƒ ê°„ì— ë­”ê°€ ì˜ë¯¸ê°€ ìˆì„ì§€ë„ ëª¨ë¥´ëŠ” ê±°ë¦¬ë¥¼ ê°€ì§€ëŠ”ë° ì´ê±´ ìš°ë¦¬ê°€ data imbalanceë¥¼ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í• ì§€ì— ëŒ€í•œ íŒíŠ¸ë¥¼ ì¤€ë‹¤.
ì˜ˆë¥¼ ë“¤ì–´ì„œ training ë°ì´í„°ì—ì„œ t1, t2ë¼ëŠ” ë‘ ê°œì˜ ì†Œìˆ˜ í´ë˜ìŠ¤ íƒ€ê¹ƒì´ ìˆë‹¤ê³  í•´ë³´ì. (ì •í™•íˆëŠ” í´ë˜ìŠ¤ëŠ” ì•„ë‹ˆì§€ë§Œ ì•”íŠ¼)
ê·¸ëŸ°ë° t1ì€ ì¸ì ‘ê°’ì´ ë§ì´ í¬ì§„ë˜ì–´ ìˆëŠ” ì˜ì—­ ì•ˆì— ìˆëŠ” ë°˜ë©´ ([t1-ğ›¥, t1+ğ›¥] ë²”ìœ„ ì•ˆì— ìƒ˜í”Œ ìˆ˜ê°€ ë§ìŒ), t2ëŠ” ê·¸ë ‡ì§€ê°€ ì•Šë‹¤.
ì´ëŸ° ê²½ìš°ì— t1ì€ t2ì™€ëŠ” ë‹¤ë¥¸ ì •ë„ì˜ ë¶ˆê· í˜•ì„ ë³´ì´ëŠ” ê²ƒì´ë‹¤.<br>
ë§ˆì§€ë§‰ìœ¼ë¡œ classificationê³¼ ë‹¬ë¦¬, íŠ¹ì • íƒ€ê¹ƒ ê°’ë“¤ì€ ë°ì´í„°ê°€ ì•„ì˜ˆ ì¡´ì¬í•˜ì§ˆ ì•Šì•„ì„œ íƒ€ê¹ƒì˜ extrapolationê³¼ interpolationì„ í•„ìš”ë¡œ í•œë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ DIRì„ ë‹¤ë£¨ëŠ” ë°©ë²• ë‘ ê°€ì§€ë¥¼ ì„ ë³´ì¸ë‹¤: **label distribution smoothing (LDS)** and **feature distribution smoothing (FDS)**.
ë‘ ì ‘ê·¼ë²•ì— ëŒ€í•œ í•µì‹¬ì ì¸ ë°œìƒì€ kernel distributionì„ ì¨ì„œ ì¸ì ‘í•œ íƒ€ê¹ƒë“¤ ê°„ì˜ ìœ ì‚¬ì„± (similarity between nearby targets)ì„ ì´ìš©í•¨ìœ¼ë¡œì¨ label spaceì™€ feature space ìƒì—ì„œ distribution smoothingì„ í•´ì£¼ëŠ” ê²ƒì´ë‹¤.
ë‘ ê¸°ìˆ  ëª¨ë‘ ê¸°ì¡´ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì— ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆê³  end-to-end ë°©ì‹ì˜ optimizationì´ ê°€ëŠ¥í•˜ë‹¤.
ë˜í•œ ì´ ê¸°ìˆ ë“¤ì€ ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ í•¨ê»˜ ì‚¬ìš©í–ˆì„ ë•Œ ë”ìš±ë” íš¨ê³¼ì ì´ë‹¤.

## 2. Related work

ëŒ€ì¶© ê¸°ì¡´ì˜ **Imbalanced Classification** (SMOTE ë“±ë“±...), **Imbalanced Regression** (SMOTE í™œìš©...) ê³¼ëŠ” ì°¨ë³„í™” ë˜ì—ˆë‹¤ëŠ” ì´ì•¼ê¸°.

## 3. Methods

<!--
### Problem settings

`{(xi, yi)}N,i=1`ì´ë¼ëŠ” training setì´ ìˆë‹¤ê³  í•˜ì. (`xi`: ì‹¤ìˆ˜ input, `yi`: ì‹¤ìˆ˜ label)
ì—¬ê¸°ì„œ label space `Y`ì™€ íƒ€ê¹ƒ ê°’ì˜ ê·¸ë£¹ ì¸ë±ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” `B` ì§‘í•©ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.
-->

### 3.1. Label Distribution Smoothing

### 3.2. Feature Distribution Smoothing

## 4. Benchmarking DIR

## 5. Conclusion
