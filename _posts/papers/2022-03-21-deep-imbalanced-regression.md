---
title: "[ë…¼ë¬¸ë¦¬ë·°] LDS, FDS (2021)"
categories: paper-review
tags: machine-learning python pytorch
toc: true
toc_sticky: true
---

# DIR ë…¼ë¬¸ & ì½”ë“œ ë¦¬ë·°

[ë…¼ë¬¸: Delving into Deep Imbalanced Regression](http://proceedings.mlr.press/v139/yang21m.html), [Github](https://github.com/YyzHarry/imbalanced-regression), [ì €ìì˜ í¬ìŠ¤íŠ¸](https://towardsdatascience.com/strategies-and-tactics-for-regression-on-imbalanced-data-61eeb0921fca)

## My Preview

### Imbalanced Regression

ë‚´ê°€ í•™ìŠµì— ì‚¬ìš©í•˜ê³  ìˆëŠ” ë°ì´í„°ëŠ” Medical dataì´ë‹¤. Medical/Clinical/Healthcare dataëŠ” ë‹¤ë¥¸ ë°ì´í„°ì— ë¹„í•´ì„œ íŠ¹íˆ ë” ë¶ˆê· í˜•(imbalanced)ì¸ ê²½ìš°ê°€ ë§ë‹¤.
ì¼ë°˜ì ìœ¼ë¡œ ì§ˆë³‘ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ ê±´ê°• ì •ë³´ëŠ” ë³‘ì— ê±¸ë¦° ì‚¬ëŒë³´ë‹¤ ê±¸ë¦¬ì§€ ì•Šì€ ê±´ê°•í•œ ì‚¬ëŒì˜ ë¹„ìœ¨ì´ í›¨ì”¬ ë†’ê¸° ë•Œë¬¸ì´ë‹¤.<br>
ì˜ˆë¥¼ ë“¤ì–´ ë³‘ë³€ ë°ì´í„°ë¼ê³  í•˜ë©´ ë¹„êµì  ë³‘ë³€ ìˆ˜ì¹˜ê°€ ì‘ì€ ìª½ìœ¼ë¡œ ì¹˜ìš°ì¹œ ë¶„í¬ í˜•íƒœì¸ right-skewed (positively skewed) distributionì„ ë³´ì´ê²Œ ëœë‹¤.

ì…ë ¥ ë°ì´í„°ê°€ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ë¥¼ ì•Œì•„ë³´ëŠ” classification taskì—ì„œì˜ imbalanced data handlingì— ëŒ€í•œ ë°©ë²•ìœ¼ë¡œëŠ”
[SMOTE](https://www.jair.org/index.php/jair/article/view/10302)(<u>S</u>ynthetic <u>M</u>inority <u>O</u>versampling <u>Te</u>chnique) ê°™ì€ ê²ƒë“¤ì´ ì•Œë ¤ì ¸ ìˆë‹¤.<br>
ê·¸ëŸ°ë° ë‚´ ì—°êµ¬ ì£¼ì œëŠ” ì…ë ¥ ë°ì´í„°ë¡œ ì´ë¯¸ì§€ë¥¼ ì£¼ê³  ì‹¤ìˆ˜ê°’ì„ ì˜ˆì¸¡í•˜ê²Œ í•˜ëŠ” *regression task*ë¼ì„œ SMOTE ê°™ì€ ë°©ë²•ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸°ê°€ ì• ë§¤í•˜ë‹¤.

**Delving into Deep Imbalanced Regression** ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ° ë‚˜ì—ê²Œ í•„ìš”í•œ ë°ì´í„° ë¶ˆê· í˜• í•´ê²° ë°©ë²•ì„ ì†Œê°œí•˜ê³  ìˆë‹¤.

## 0. Abstract

ê¸°ì¡´ì˜ ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ì‹ë“¤ì€ ë²”ì£¼í˜• ë°ì´í„° (target with categorical indices)ë“¤ì˜ ë¶ˆê· í˜• ë¬¸ì œì— ì´ˆì ì„ ë§ì¶”ê³  ìˆë‹¤. (ì£¼ë¡œ classification taskì—ì„œì˜ ë°ì´í„° ë¶ˆê· í˜•)
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì—°ì†ì ì¸ ê°’ë“¤ë¡œ êµ¬ì„±ëœ ë°ì´í„°ë¥¼ í•™ìŠµí•  ë•Œì˜ ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œ (Deep Imbalanced Regression; DIR)ë¥¼ í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•œë‹¤.
ë²”ì£¼í˜• ë°ì´í„°ì™€ ì—°ì†í˜• ë°ì´í„°ì˜ ë³¸ì§ˆì ì¸ ì°¨ì´ì ì—ì„œ ì°©ì•ˆí•˜ì—¬ labelê³¼ featureì— ëŒ€í•œ distribution smoothing ê¸°ë²•ì„ ì œì•ˆí•˜ê³  ìˆê³ , ì´ëŠ” imbalanced <u>regression problem</u>ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë‹¤.

## 1. Introduction

ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œëŠ” í¸ì¬í•œë‹¤. ì´ í˜„ìƒì€ ê¸°ê³„ í•™ìŠµì—ë„ í° ë°©í•´ìš”ì†Œê°€ ë˜ê¸° ë•Œë¬¸ì— ì´ë¯¸ ì˜ˆì „ë¶€í„° ì´ëŸ° ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ë“¤ì´ ë§ì´ ì œì•ˆë˜ì—ˆë‹¤.

ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ì˜ ì†”ë£¨ì…˜ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ì‚°ì (discrete)ìœ¼ë¡œ í´ë˜ìŠ¤(class)ê°€ ë‚˜ë‰˜ì–´ ìˆëŠ”, ì´ë¥¸ë°” classification ë¬¸ì œì—ë§Œ ì£¼ë ¥í•˜ê³  ìˆë‹¤.
ì •ì‘ í˜„ì‹¤ ì„¸ê³„ì˜ ë°ì´í„°ëŠ” ì—°ì†ì ì´ê³  ë²”ìœ„ê°€ ë¬´í•œí•œ ê°’ë“¤ë¡œ ì´ë¤„ì ¸ ìˆìœ¼ë©° ì´ëŸ¬í•œ ì—°ì†í˜• ë°ì´í„° ì•ˆì—ì„œë„ ë¶ˆê· í˜• ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°ê°€ ë§ë‹¤.
ê·¸ ëŒ€í‘œì ì¸ ì˜ˆì‹œê°€ ë‚´ê°€ ê°€ì§€ê³  ìˆëŠ” ê²ƒê³¼ ê°™ì€ medical/clinical dataì´ë‹¤.

imbalanced regressionì€ imbalanced classificationì´ ê°€ì§€ëŠ” ë¬¸ì œì ê³¼ëŠ” ë‹¤ë¥¸ ë¬¸ì œì ë“¤ì´ ìˆëŠ”ë°, ì—¬ê¸°ì„  ëŒ€í‘œì ìœ¼ë¡œ 3ê°€ì§€ë¥¼ ë“¤ê³  ìˆë‹¤.

ë¨¼ì € ì—°ì†ì ì¸ íƒ€ê¹ƒ ê°’ë“¤ì€ (ì•ìœ¼ë¡œ íƒ€ê¹ƒ(target)ì´ë¼ê³  í•˜ë©´ ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ì— í•™ìŠµì‹œí‚¬ ëŒ€ìƒì„ ì§€ì¹­í•œë‹¤.) í´ë˜ìŠ¤ ê°„ ëšœë ·í•œ ê²½ê³„ê°€ ì—†ê¸° ë•Œë¬¸ì— re-samplingì´ë‚˜ re-weighting ê°™ì€ ì „í†µì ì¸ imbalanced classification ë°©ì‹ì„ ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸°ì—” ì• ë§¤í•´ì§„ë‹¤.<br>
ì´ê²Œ ë¬´ìŠ¨ ë§ì´ëƒ í•˜ë©´, ì‹¤ìˆ˜ê°’ ë°ì´í„°ëŠ” classificationìœ¼ë¡œ ì¹˜ë©´ ê·¸ ê°’ ìì²´ë¡œ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ê°€ ë˜ì–´ë²„ë¦°ë‹¤.
SMOTE ê°™ì€ re-sampling ê¸°ë²•ì€ ë³´í†µ ì†Œìˆ˜ í´ë˜ìŠ¤ ìƒ˜í”Œ ì‚¬ì´ì‚¬ì´ì— ì„ì˜ì˜ ê°’ì„ ìƒˆë¡œ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì´ë¤„ì§€ëŠ”ë°, ì—°ì† ë°ì´í„°ëŠ” ê·¸ ê°’ ìì²´ë¡œ í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë‹ˆê¹Œ í´ë˜ìŠ¤ 'ì‚¬ì´ì‚¬ì´' ê³µê°„ì„ ì •ì˜í•˜ê¸° ë§¤ìš° ì–´ë ¤ì›Œì§€ëŠ” ê²ƒì´ë‹¤.

ë°©ê¸ˆ ì „ ì–˜ê¸°ì™€ ê°™ì€ ë§¥ë½ì—ì„œ ì—°ì†ì ì¸ íƒ€ê¹ƒ ë°ì´í„°ë“¤ì€ íƒ€ê¹ƒ ê°’ë“¤ ê°„ ê±°ë¦¬ì—ë„ ì˜ë¯¸ê°€ ìˆìœ¼ë©°, ì´ê±´ ë°ì´í„° ë¶ˆê· í˜•ì„ ì–´ë–»ê²Œ í•´ì„í•´ì•¼ í• ì§€ì— ëŒ€í•œ íŒíŠ¸ë¥¼ ì¤€ë‹¤.<br>
ì˜ˆë¥¼ ë“¤ì–´ì„œ training ë°ì´í„°ì…‹ì—ì„œ t1, t2ë¼ëŠ” ë‘ ê°œì˜ íƒ€ê¹ƒì´ ìˆë‹¤ê³  í•´ë³´ì.<br>
![Challenge 2](/assets/images/220321/dir_challenge_2.png)<br>
<span style="font-size:xx-small">[ì¶œì²˜: ì €ìì˜ í¬ìŠ¤íŠ¸](https://towardsdatascience.com/strategies-and-tactics-for-regression-on-imbalanced-data-61eeb0921fca)</span><br>
ì—¬ê¸°ì„œ t1 ì£¼ë³€ì˜ ê°’ë“¤ì€ ë¹ˆë„ê°€ ë†’ì€ ë°˜ë©´ ([t1-âˆ†, t1+âˆ†] ë²”ìœ„ ì•ˆì— ìƒ˜í”Œ ìˆ˜ê°€ ë§ìŒ) t2ëŠ” ê·¸ë ‡ì§€ê°€ ì•Šì€ ê²½ìš°,
ë¹„ë¡ t1ì˜ ê°œìˆ˜ì™€ t2ì˜ ê°œìˆ˜ê°€ ê°™ì„ì§€ë¼ë„ ë‘ íƒ€ê¹ƒì˜ ë¶ˆê· í˜• ì •ë„ê°€ ì„œë¡œ ê°™ë‹¤ê³  í•  ìˆ˜ ì—†ë‹¤.

ë§ˆì§€ë§‰ìœ¼ë¡œ classificationê³¼ ë‹¬ë¦¬, ì–´ë–¤ íƒ€ê¹ƒ ê°’ë“¤ì€ ë°ì´í„°ê°€ ì•„ì˜ˆ ì¡´ì¬í•˜ì§ˆ ì•Šì•„ì„œ íƒ€ê¹ƒì˜ extrapolationì´ë‚˜ interpolationì„ í•„ìš”ë¡œ í•œë‹¤.<br>
![Challenge 3](/assets/images/220321/dir_challenge_3.png)<br>
<span style="font-size:xx-small">ì¶œì²˜: ì €ìì˜ í¬ìŠ¤íŠ¸</span>

ì´ ë…¼ë¬¸ì—ì„œëŠ” ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ DIRì„ ë‹¤ë£¨ëŠ” ë°©ë²• ë‘ ê°€ì§€ë¥¼ ì„ ë³´ì¸ë‹¤: **label distribution smoothing (LDS)** and **feature distribution smoothing (FDS)**.
ë‘ ì ‘ê·¼ë²•ì— ëŒ€í•œ í•µì‹¬ì ì¸ ë°œìƒì€ kernel distributionì„ í™œìš©í•´ì„œ ì¸ì ‘í•œ íƒ€ê¹ƒë“¤ ê°„ì˜ ìœ ì‚¬ë„ (similarity)ë¥¼ ì´ìš©í•¨ìœ¼ë¡œì¨ label spaceì™€ feature space ìƒì—ì„œ distribution smoothingì„ í•´ì£¼ëŠ” ê²ƒì´ë‹¤.
ë‘ ê¸°ìˆ  ëª¨ë‘ ê¸°ì¡´ ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤ì— ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆê³  end-to-end ë°©ì‹ì˜ optimizationì´ ê°€ëŠ¥í•˜ë‹¤.
ë˜í•œ ì´ ê¸°ìˆ ë“¤ì€ ë‹¤ë¥¸ ë°©ë²•ë“¤ì„ í•¨ê»˜ ì‚¬ìš©í–ˆì„ ë•Œ ë”ìš±ë” íš¨ê³¼ì ì´ë‹¤.

## 2. Related work

ëŒ€ì¶© ê¸°ì¡´ì˜ **Imbalanced Classification** (SMOTE ë“±ë“±...), **Imbalanced Regression** (SMOTE í™œìš©...) ê³¼ëŠ” ì°¨ë³„í™” ë˜ì—ˆë‹¤ëŠ” ì´ì•¼ê¸°.

## 3. Methods

<!--
### Problem settings

`{(xi, yi)}N,i=1`ì´ë¼ëŠ” training setì´ ìˆë‹¤ê³  í•˜ì. (`xi`: ì‹¤ìˆ˜ input, `yi`: ì‹¤ìˆ˜ label)
ì—¬ê¸°ì„œ label space `Y`ì™€ íƒ€ê¹ƒ ê°’ì˜ ê·¸ë£¹ ì¸ë±ìŠ¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” `B` ì§‘í•©ì„ ì‚¬ìš©í•  ê²ƒì´ë‹¤.
-->

### 3.1. Label Distribution Smoothing

LDS ì„¤ëª…ì— ì•ì„œ classificationê³¼ regressionì˜ ì°¨ì´ì ì— ëŒ€í•´ì„œ ì˜ˆì‹œë¥¼ ê°€ì§€ê³  ì„¤ëª…í•œë‹¤.

> Motivating Example.

label ë²”ìœ„ê°€ 0~99ë¡œ ê°™ê³ , label density distributionì´ ê°™ì€ `classification ë°ì´í„°ì…‹`ê³¼ `regression ë°ì´í„°ì…‹`ì„ ê°€ì§€ê³  ì„¤ëª…í•´ë³´ê² ë‹¤.<br>
ì°¸ê³ ë¡œ ì´ ë…¼ë¬¸ì—ì„œëŠ” í™•ì‹¤í•œ ë¹„êµë¥¼ ìœ„í•´ ë‘ ë°ì´í„°ì…‹ì˜ label density distributionì„ ì¸ìœ„ì ìœ¼ë¡œ ë˜‘ê°™ì´ ë§ì¶°ì¤¬ë‹¤.

![Figure 2](/assets/images/220321/dir_figure_2.png)

íŒŒë€ìƒ‰ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œì‹œëœ ìœ„ìª½ ê·¸ë¦¼ì€ ê° ë°ì´í„°ì…‹ì˜ ë¶„í¬ë¥¼ ë³´ì—¬ì£¼ê³ , ë¹¨ê°„ìƒ‰ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œì‹œëœ ì•„ë˜ìª½ ê·¸ë¦¼ì€ í•™ìŠµ í›„ì˜ label ë³„ test errorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.<br>
*Figure 2.(a)*ë¥¼ ë³´ë©´ error distributionê³¼ label density distributionì— negative correlationì´ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ë°˜ë©´ì— ì—°ì† ë°ì´í„°ì…‹ì— ëŒ€í•œ ê·¸ë˜í”„ì¸ *Figure 2.(b)*ë¥¼ ë³´ë©´ *Figure 2.(a)*ì— ë¹„í•´ì„œ error distributionì´ í›¨ì”¬ smoothí•˜ë©° ìƒëŒ€ì ìœ¼ë¡œ label density distributionê³¼ì˜ correlationì´ ì—†ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.

ì¼ë°˜ì ìœ¼ë¡œ re-weightingê³¼ ê°™ì€ imbalanced learning methodëŠ” *ì‹¤ì¦ì ì¸ (empirical)* label density distributionì˜ ë¶ˆê· í˜•ì— ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ì‹ìœ¼ë¡œ ì‘ìš©í•œë‹¤.
ì´ëŸ¬í•œ ë°©ì‹ì€ test error distributionê³¼ label density distribution ê°„ì˜ correlationì´ í™•ì‹¤í•˜ê²Œ ë³´ì´ëŠ” í´ë˜ìŠ¤ ë°ì´í„°ì— ì ìš©í•  ë•ŒëŠ” íš¨ê³¼ì ì´ì§€ë§Œ,
ì—°ì†ì ì¸ ë°ì´í„°ì— ëŒ€í•´ì„  í…ŒìŠ¤íŠ¸ ì—ëŸ¬ê°€ ì‹¤ì œ í•™ìŠµ ëª¨ë¸ì—ì„œ ë³´ì´ëŠ” ê²ƒë§Œí¼ì˜ ë¶ˆê· í˜•ì„ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ê·¸ëŒ€ë¡œ ì ìš©í•˜ê¸°ì— ì ì ˆì¹˜ ì•Šë‹¤.

> LDS for Imbalanced Data Density Estimation.

ì•ì„  ì˜ˆì‹œì—ì„œ ì—°ì†ì ì¸ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œëŠ” empirical label density distributionì´ ì‹¤ì œ label density distributionì„ ë°˜ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ë³´ì˜€ë‹¤. ì´ê²ƒì€ ì¸ì ‘í•œ label ë°ì´í„° ìƒ˜í”Œë“¤ ê°„ì˜ ì˜ì¡´ì„±(dependence) ë•Œë¬¸ì´ë‹¤.
ë°ì´í„° ìƒ˜í”Œ ìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ë¹„êµì  ì´ì›ƒí•œ ê°’ë“¤ ì¤‘ì—ì„œ ìƒ˜í”Œ ìˆ˜ê°€ ë†’ì€ ë°ì´í„°ì— ì˜ì¡´í•˜ì—¬ í•™ìŠµí•˜ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.<br>
Label Distribution Smoothing (LDS)ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì—°ì†ì ì¸ íƒ€ê¹ƒ ë°ì´í„° ê°„ ë¶ˆê· í˜•ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ê¸° ìœ„í•´ kernel densityì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì´ë‹¤.

LDSëŠ” symmetric kernelê³¼ empirical density distributionì„ ê°€ì§€ê³  convolutionì„ í•´ì„œ ì¸ì ‘í•œ ë°ì´í„° ìƒ˜í”Œ ë¶„í¬ê°€ ë¶€ë“œëŸ¬ì›Œì§„ kernel-smoothed ë²„ì „ì„ ë§Œë“¤ì–´ë‚¸ë‹¤. ì¦‰ smoothingì´ë¼ëŠ” ê±´ í•„í„°ë§ ì‘ì—…ì´ë‹¤.<br>
ì—¬ê¸°ì„œ symmetric kernelì´ë¼ëŠ” ê±´ `k(y, y')=k(y', y)`ì™€ `âˆ‡y(k(y, y'))+âˆ‡y(k(y', y))=0, âˆ€y,y'âˆˆY (Y: label space)`ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë“  kernelì„ ì˜ë¯¸í•œë‹¤.
Gaussian kernelê³¼ Laplace kernelì€ symmetric kernelì˜ ì¼ì¢…ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆê³  `k(y, y')=yy'` ê°™ì€ ê±´ symmetric kernelì´ ì•„ë‹ˆë‹¤.
LDSëŠ” ê²°êµ­ *effective label density distribution*ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•˜ê²Œ ëœë‹¤:

![Formula 1](/assets/images/220321/dir_formula_1.png)

`p(y)`ëŠ” training dataì—ì„œì˜ label ê°œìˆ˜ê³ , `p~(y')`ëŠ” y' labelì˜ effective densityì´ë‹¤.

![Figure 3](/assets/images/220321/dir_figure_3.png)

*Figure 3*ì€ LDSì˜ label density distribution í•„í„°ë§ ë°©ì‹ê³¼ í•„í„°ë§ í›„ test errorì™€ì˜ correlationì„ ê°„ë‹¨íˆ ë³´ì—¬ì¤€ë‹¤.

```python
from scipy.ndimage import gaussian_filter1d
from scipy.signal.windows import triang
...

def get_lds_kernel_window(kernel, ks, sigma):
    assert kernel in ['gaussian', 'triang', 'laplace']
    half_ks = (ks - 1) // 2
    if kernel == 'gaussian':
        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks
        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))
    elif kernel == 'triang':
        kernel_window = triang(ks)
    else:
        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)
        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))

    return kernel_window
```

ë…¼ë¬¸ì—ì„œ ì œê³µí•˜ëŠ” ê¹ƒí—™ ë ˆí¬ì—ì„œ ìœ„ì˜ [LDS kernelì„ ìƒì„±í•˜ëŠ” ì½”ë“œ](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/utils.py#L110)ë¥¼ ê°€ì ¸ì™”ë‹¤.<br>
ì½”ë“œë¥¼ ë³´ë©´ ìœ„ì—ì„œ symmetric kernelì´ë¼ê³  ì–¸ê¸‰í•œ gaussian í•„í„°ì™€ laplace í•„í„°ë¥¼ êµ¬í˜„í•˜ê³  ìˆë‹¤. triangì€ triangle window, ì¦‰ ì‚¼ê°í˜• í•„í„°ì´ë‹¤.
gaussian í•„í„°ì™€ ì‚¼ê°í˜• í•„í„°ëŠ” ì„¤ëª…í•˜ì§€ ì•Šì•„ë„ ê·¸ë¦¼ì„ ë³´ë©´ ê·¸ëƒ¥ ë´ë„ ëª¨ë‘ symmetric(ëŒ€ì¹­)ì´ë¼ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤:

![Gaussian](/assets/images/220321/Gaussian_Filter.png)<br>
<span style="font-size:xx-small">[ì¶œì²˜: Wikipedia](https://en.wikipedia.org/wiki/Gaussian_filter)</span>

![Triangle](/assets/images/220321/scipy-signal-windows-triang.png)<br>
<span style="font-size:xx-small">[ì¶œì²˜: Scipy docs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.triang.html)</span>

laplace í•„í„°ëŠ” discrete domainì—ì„œ ì“°ì´ëŠ” í•„í„°ë¡œ, ëŒ€ì¶© ì´ëŸ° í˜•íƒœë¥¼ ìƒê°í•˜ë©´ ëœë‹¤:

![Laplace](/assets/images/220321/laplace_filter.png)<br>
<span style="font-size:xx-small">[ì¶œì²˜: Wikipedia](https://en.wikipedia.org/wiki/Discrete_Laplace_operator)</span>

ì¦‰ í•œê°€ìš´ë°ì˜ ì›ì†Œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëŒ€ì¹­ì¸ matrixì´ë‹¤.

<span style="color:blue">
<u>ê·¸ëƒ¥ ë‚´ ìƒê°</u><br>
scipy.signal.windowsì—ì„œëŠ” ì‚¼ê°í˜• ë§ê³ ë„ ë‹¤ì–‘í•œ ëª¨ì–‘ì˜ symmetricí•œ í•„í„°ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤.<br>
ì»¤ìŠ¤í„°ë§ˆì´ì§•ì„ ìœ„í•´ ë‹¤ë¥¸ í•„í„°ë“¤ë„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆì„ê¹Œ?<br>
</span>

LDSë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìœ¼ë©°,

```python
from collections import Counter
from scipy.ndimage import convolve1d
from utils import get_lds_kernel_window

# preds, labels: [Ns,], "Ns" is the number of total samples
preds, labels = ..., ...
# assign each label to its corresponding bin (start from 0)
# with your defined get_bin_idx(), return bin_index_per_label: [Ns,]
bin_index_per_label = [get_bin_idx(label) for label in labels]

# calculate empirical (original) label distribution: [Nb,]
# "Nb" is the number of bins
Nb = max(bin_index_per_label) + 1
num_samples_of_bins = dict(Counter(bin_index_per_label))
emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(Nb)]

# lds_kernel_window: [ks,], here for example, we use gaussian, ks=5, sigma=2
lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=5, sigma=2)
# calculate effective label distribution: [Nb,]
eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')
```

(bin indexëŠ” ë°ì´í„° ê°’ë“¤ì˜ ë²”ìœ„ì— êµ¬ê°„(bin)ì„ ë‚˜ëˆ ì„œ êµ¬ê°„ë§ˆë‹¤ í• ë‹¹í•´ì£¼ëŠ” idë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.)

ìœ„ì—ì„œ êµ¬í•œ effective label distribution ì¶”ì •ì¹˜ë¥¼ ê°€ì§€ê³  loss re-weightingì„ í•  ìˆ˜ ìˆë‹¤.
í•„í„°ë§ì„ í†µí•´ ë§Œë“¤ì–´ì§„ smoothed label distributionì´ ì¢€ ë” ì´ìƒì ì¸ ë°ì´í„° ë¶„í¬ë¼ëŠ” ê°€ì • í•˜ì—, ì›ë˜ì˜ ì •ë‹µ ëŒ€ì‹  ì´ ë¶„í¬ì— ë§ì¶°ì„œ loss ê°’ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.
ì½”ë“œ ìƒìœ¼ë¡œëŠ” loss functionì— ê° íƒ€ê¹ƒì— LDSë¥¼ ì·¨í•œ label density ì˜ˆì¸¡ì¹˜ì˜ ì—­ìˆ˜ë¥¼ ê³±í•´ì„œ re-weightingì„ í•¨ìœ¼ë¡œì¨ cost-sensitiveí•˜ê²Œ [loss functionì„ re-weighting](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/loss.py#L5) í•˜ë„ë¡ êµ¬í˜„ë˜ì–´ ìˆë‹¤.

```python
from loss import weighted_mse_loss

# Use re-weighting based on effective label distribution, sample-wise weights: [Ns,]
eff_num_per_label = [eff_label_dist[bin_idx] for bin_idx in bin_index_per_label]
weights = [np.float32(1 / x) for x in eff_num_per_label]

# calculate loss
loss = weighted_mse_loss(preds, labels, weights=weights)
```

```python
def weighted_mse_loss(inputs, targets, weights=None):
    loss = (inputs - targets) ** 2
    if weights is not None:
        loss *= weights.expand_as(loss)
    loss = torch.mean(loss)
    return loss
```

<!--ì½”ë“œë¥¼ í†µí•´ LDSëŠ” í•™ìŠµ ì¤‘ ì˜ˆì¸¡ê°’ì˜ ë¶„ì‚° ì •ë„ë¥¼ ê°€ì§€ê³  lossë¥¼ ë‹¤ì‹œ ê³„ì‚°í•˜ëŠ” í˜•íƒœë¡œ ì´í•´í•˜ì˜€ë‹¤.-->

### 3.2. Feature Distribution Smoothing

ì•ì„  ì˜ˆì‹œë“¤ì„ í†µí•´ ì´ì œ target spaceì—ì„œì˜ ì—°ì†ì„±ì€ feature spaceì—ì„œì˜ ì—°ì†ì„±ì— ëŒ€ì‘í•´ì•¼ í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.
ì¦‰ í•™ìŠµ ëª¨ë¸ì´ ì œëŒ€ë¡œ ëŒì•„ê°€ê³  ë°ì´í„° ë¶„í¬ê°€ ê· í˜• ì¡í˜€ ìˆë‹¤ë©´, ì¸ì ‘ íƒ€ê¹ƒì— ëŒ€í•œ feature í†µê³„ë¥¼ ë´¤ì„ ë•Œ ì¸ì ‘í•œ ì• ë“¤ë¼ë¦¬ëŠ” ê±°ì˜ ìœ ì‚¬í•œ featureë¥¼ ë½‘ì•„ë‚¼ ê²ƒì´ë‹¤.

> Motivating Example.

ì˜ˆì‹œë¡œ í•™ìŠµëœ featureì˜ í†µê³„ë¥¼ ë¶„ì„í•´ë³´ë ¤ê³  í•œë‹¤.
ê° binì— ì†í•œ íƒ€ê¹ƒ ë°ì´í„°ì— ëŒ€í•œ í‰ê· (`ğœ‡`)ê³¼ í‘œì¤€í¸ì°¨(`ğœ`)ë¥¼ ê³„ì‚°í•œ ê²ƒì„ ![feature bin denotement](/assets/images/220321/dir_feature_bin.png){: width="100" height="100"}ë¼ê³  í–ˆì„ ë•Œ, (ì—¬ê¸°ì„œ `b`ëŠ” íƒ€ê¹ƒ ê°’ì˜ group index ë˜ëŠ” bin indexë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.)
*Figure 4*ëŠ” íƒ€ê¹ƒ ê°’ 30ì„ ê¸°ì¤€ìœ¼ë¡œ (`b` ê¸°ì¤€ê°’ = 30) ê° íƒ€ê¹ƒì— ëŒ€í•œ feature ë²¡í„° ![feature bin denotement](/assets/images/220321/dir_feature_bin.png){: width="100" height="100"}ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„(cosine similarity)ë¥¼ ë³´ì—¬ì¤€ë‹¤.

ì°¸ê³ ë¡œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ëŠ” ë‘ ë²¡í„° ê°„ì˜ ë‚´ì ì„ í†µí•´ ë‘ ë²¡í„°ê°€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ê³„ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.
ìœ ì‚¬ë„ê°€ 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•œ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆê³  -1ì— ê°€ê¹Œìš°ë©´ ê±°ì˜ ìœ ì‚¬í•˜ì§€ë§Œ ë°©í–¥ë§Œ ë°˜ëŒ€ì¸ ê²ƒì¸ë°, ì˜ˆì œì—ì„œ ë‹¤ë£¨ê³  ìˆëŠ” ë°ì´í„°ëŠ” ì‚¬ëŒì˜ ë‚˜ì´ ë°ì´í„°ë¼ ì–‘ìˆ˜ ê°’ë§Œ ìˆìœ¼ë¯€ë¡œ ìœ ì‚¬ë„ í†µê³„ ì—­ì‹œ ì–‘ìˆ˜ë§Œ ë‚˜ì˜¬ ê²ƒì´ë‹¤.
![cosine similarity](/assets/images/220321/dir_cosine_similarity.png)<br>
<span style="font-size:xx-small">[ì¶œì²˜: ë”¥ëŸ¬ë‹ì„ ì´ìš©í•œ ìì—°ì–´ ì²˜ë¦¬ ì…ë¬¸](https://wikidocs.net/24603)</span>

![Figure 4](/assets/images/220321/dir_figure_4.png)

í•‘í¬ìƒ‰, ë…¸ë€ìƒ‰, íŒŒë€ìƒ‰ìœ¼ë¡œ í‘œì‹œëœ ê³³ì€ ì°¨ë¡€ëŒ€ë¡œ ê°œìˆ˜ê°€ ë§¤ìš° ì ì€ ë°ì´í„°, ê°œìˆ˜ê°€ ë³´í†µì¸ ë°ì´í„°, ê°œìˆ˜ê°€ ë§ì€ ë°ì´í„°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
ìœ„ìª½ì˜ í‰ê·  ê·¸ë˜í”„ì—ì„œ í•‘í¬ìƒ‰ êµ¬ê°„ ì¤‘ì—ì„œë„ íŠ¹íˆ 0~6 ë²”ìœ„ì˜ ìœ ì‚¬ë„ê°€ ê±°ì˜ 1ì¸ ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ë‹¤ì‹œ ë§í•´ì„œ 0~6 ë²”ìœ„ ì•ˆì— ìˆëŠ” ë°ì´í„°ê°€ ì¶©ë¶„íˆ ë§ì§€ ì•Šë‹¤ ë³´ë‹ˆ ì´ ë°ì´í„° ì¤‘ ê°€ì¥ ë§ì€ ë¶„í¬ë¥¼ ì°¨ì§€í•˜ëŠ” ëŒ€ëµ 30ì— ê°€ê¹Œìš´ ê°’ì„ ë±‰ì–´ë‚´ëŠ” ê²ƒì´ë‹¤.

> FDS Algorithm.

feature distribution smoothing (FDS)ëŠ” ë°”ë¡œ ì´ëŸ° ì ì—ì„œ ê³ ì•ˆí•œ ê²ƒì´ë‹¤.<br>
FDSëŠ” feature ë²¡í„°ë¥¼ í•„í„°ë§ í•´ì„œ í¸í–¥ëœ feature ë¶„í¬ë¥¼ ë³´ì •í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ì´ë§ì¸ì¦‰ìŠ¨ ì¹˜ìš°ì¹œ ë°ì´í„°, íŠ¹íˆ <u>ê°œìˆ˜ê°€ ë¶€ì¡±í•œ ë°ì´í„°</u>ì˜ ì˜ˆì¸¡ì¹˜ë¥¼ ë³´ì •í•˜ê² ë‹¤ëŠ” ëœ»ì´ë‹¤.<br>
ë¨¼ì € feature space `z`ì—ì„œ ê° binì˜ feature í‰ê·  ë° ë¶„ì‚°ì„ ê³„ì‚°í•˜ê³ , ë¶„ì‚° ê°’ì„ featureë“¤ì˜ covariance (ê³µë¶„ì‚°)ìœ¼ë¡œ ëŒ€ì¹˜ì‹œì¼œì„œ feature ë²¡í„°ë¥¼ ì •ê·œí™”(normalize)í•œë‹¤.<br>
![formula 2,3](/assets/images/220321/dir_formula_2-3.png)<br>
ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë°”ë€ feature ë²¡í„°ì— í•„í„°ë¥¼ ì…íŒë‹¤.<br>
![formula 4,5](/assets/images/220321/dir_formula_4-5.png)<br>
í•„í„°ë¥¼ ì…íˆê¸° ì „ì˜ `{Î¼b, Î£b}`ì™€ `{Î¼ Ìƒb, Î£b}` ë‘˜ì„ ì´ìš©í•´ì„œ <u>standard whitening and re-coloring</u> ì ˆì°¨ë¥¼ ê±°ì¹œë‹¤.
(ì´ ë¶€ë¶„ì€ ì˜ ëª¨ë¥´ëŠ” ê°œë…ì´ë¼ì„œ í•´ë‹¹ ë…¼ë¬¸ì„ ë”°ë¡œ ì •ë¦¬í•´ë´ì•¼ í•  ë“¯)<br>
![formula 6](/assets/images/220321/dir_formula_6.png)

*Figure 5*ëŠ” ì´ëŸ¬í•œ ê³¼ì •ì„ ì§‘ì•½ì ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤.

![Figure 5](/assets/images/220321/dir_figure_5.png)

FDSëŠ” [ë§ˆì§€ë§‰ feature mapì„ ë½‘ì•„ë‚´ëŠ” layer ë‹¤ìŒì— feature ë³´ì • layer, ì¦‰ FDS layerë¥¼ ë¼ì›Œë„£ëŠ” ë°©ì‹](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/resnet.py#L144)ìœ¼ë¡œ êµ¬í˜„í•œë‹¤.

```python
from fds import FDS

config = dict(feature_dim=..., start_update=0, start_smooth=1, kernel='gaussian', ks=5, sigma=2)

def Network(nn.Module):
    def __init__(self, **config):
        super().__init__()
        self.feature_extractor = ...
        self.regressor = nn.Linear(config['feature_dim'], 1)  # FDS operates before the final regressor
        self.FDS = FDS(**config)

    def forward(self, inputs, labels, epoch):
        features = self.feature_extractor(inputs)  # features: [batch_size, feature_dim]
        # smooth the feature distributions over the target space
        smoothed_features = features    
        if self.training and epoch >= config['start_smooth']:
            smoothed_features = self.FDS.smooth(smoothed_features, labels, epoch)
        preds = self.regressor(smoothed_features)

        return {'preds': preds, 'features': features}
```

í•™ìŠµì„ í•  ë•ŒëŠ” ë§¤ epochë§ˆë‹¤ `{ğœ‡b,ğ›´b}`ì˜ [*momentum update*](https://github.com/YyzHarry/imbalanced-regression/blob/main/imdb-wiki-dir/train.py#L280)ë¥¼ í•´ì¤Œìœ¼ë¡œì¨ ë³´ë‹¤ ì•ˆì •ì ì´ê³  ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤.

```python
model = Network(**config)

for epoch in range(num_epochs):
    for (inputs, labels) in train_loader:
        # standard training pipeline
        ...

    # update FDS statistics after each training epoch
    if epoch >= config['start_update']:
        # collect features and labels for all training samples
        ...
        # training_features: [num_samples, feature_dim], training_labels: [num_samples,]
        training_features, training_labels = ..., ...
        model.FDS.update_last_epoch_stats(epoch)
        model.FDS.update_running_stats(training_features, training_labels, epoch)
```

## 4. Benchmarking DIR

ì´ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•œ ë°©ë²•ì„ ì–´ë–»ê²Œ ì‹œí—˜í–ˆëŠ”ì§€ì— ëŒ€í•´ì„œ ì„¤ëª…í•˜ê³  ìˆë‹¤.<br>
*IMDB-WIKI* ë°ì´í„°ì…‹ì—ì„œ bin ê°’ì€ 0~7149 ì¤‘ì— ìˆìœ¼ë©° validation ë° test setì˜ ë°ì´í„° ë¶„í¬ê°€ ê³ ë¥´ê²Œë” ì§ì ‘ êµ¬ì„±í•˜ì˜€ë‹¤.

![Table 1](/assets/images/220321/dir_table_1.png)

*Table 1*ì—ì„œ `Vanilla`ëŠ” imbalance handlingì„ ì•„ì˜ˆ ì•ˆ í•œ ê²ƒ,
`Focal-R`ì€ classificationì—ì„œ ì“°ì´ëŠ” `Focal loss`ë¼ëŠ” loss í•¨ìˆ˜ì˜ regression ë²„ì „,
`RRT`ëŠ” featureì™€ classifierë¥¼ ë¶„ë¦¬ì‹œì¼œì„œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ ì„œ í•™ìŠµì‹œí‚¤ëŠ” ê¸°ë²•ì¸ `Two-stage training`ì˜ regression ë²„ì „ì´ë‹¤.
`INV`ì™€ `SQINV`ëŠ” ê°ê° inverse-frequency weighting variant, square-root weighting variantë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

ì € í‘œë¥¼ ë³´ë©´ LDS, FDS, ê·¸ë¦¬ê³  LDSë‘ FDSë¥¼ ë‘˜ ë‹¤ ì ìš©í–ˆì„ ë•Œ ì•„ë¬´ê²ƒë„ ì ìš© ì•ˆ í–ˆì„ ë•Œë³´ë‹¤ MAEê°€ ì‘ìœ¼ë¯€ë¡œ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë” ì¢‹ì•„ì¡Œë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤.
