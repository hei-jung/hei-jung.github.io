---
title: "[ë…¼ë¬¸ë¦¬ë·°] Ranking Loss"
categories: paper-review
tags: machine-learning python pytorch
toc: true
toc_sticky: true
---

# Ranking Loss ê´€ë ¨ ë…¼ë¬¸ ë¦¬ë·°

"Brain Age Estimation From MRI Using Cascade Networks With Ranking Loss" [1]ì—ì„œ ë‚˜ì˜¨ Ranking Loss ê°œë… ë° ì½”ë“œ ì´í•´í•´ ë³´ê¸°.

## Ranking Loss ê°œë…

chronological age (subjectì˜ ì‹¤ì œ ë‚˜ì´)ë¥¼ y, estimated brain age (ë‡Œ ë‚˜ì´ ì˜ˆì¸¡ê°’)ë¥¼ y hat(y^)ì´ë¼ê³  í•  ë•Œ, ë‘ ë°ì´í„° ìƒ˜í”Œ ê°„ ê´€ê³„ ì¦‰, age ê°„ì˜ ì°¨ì´ê°€ ì¤‘ìš”í•˜ë‹¤ëŠ” ì ì„ ì´ìš©í•˜ì—¬ age difference lossë¥¼ (5)ë²ˆ ì‹ê³¼ ê°™ì´ ë‘ ë°ì´í„°ì˜ estimated brain age differenceì™€ true age differenceì— ëŒ€í•œ MSEë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. (Np: (i, j) ë°ì´í„° ìŒì˜ ê°œìˆ˜)

$$
L_d = 1/N_p\sum_{(i,j)} (\widehat(y_i)-\widehat(y_j)-(y_i-y_j))^2     (5)
$$

ì´ë•Œ Ldê°€ ë‘ ìƒ˜í”Œì— ëŒ€í•œ ranking lossì´ë‹¤. age differenceì˜ ë¶€í˜¸ê°€ ê³§ ë‘ ê°œ ageì˜ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì´ë‹¤. LdëŠ” age differenceì˜ í¬ê¸°ì™€ ë¶€í˜¸ ëª¨ë‘ ê³ ë ¤í•œë‹¤.

ì°¸ê³ ë¡œ í†µê³„í•™ì—ì„œ Spearman's rank correlation coefficient (SRCC; ìŠ¤í”¼ì–´ë§Œ ìƒê´€ê³„ìˆ˜)ë¼ëŠ” ê²Œ ìˆë‹¤.<br>
Spearman ìƒê´€ ê³„ìˆ˜ëŠ” ë‘ ë³€ìˆ˜ì˜ ìœ ì‚¬ ì •ë„ë¥¼ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ë„êµ¬ë¡œ, rank variableì˜ Pearson ìƒê´€ ê³„ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê²ƒìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ ì‹ì´ ì•„ë˜ì™€ ê°™ë‹¤.

$$
r_r = \cov (\Rank(\hat(y)), \Rank(y)) / \sigma_{\Rank(\hat(y))}\sigma_{\Rank(y)}     (6)
$$

RankëŠ” ë‘ ë³€ìˆ˜ì˜ ìˆœì„œë¥¼ ê²°ì •í•˜ëŠ” rank operatorì´ê³ , cov(Rank(y^), Rank(y))ëŠ” ë‘ ê°œì˜ rank variableì— ëŒ€í•œ ê³µë¶„ì‚° (covariance), ğœëŠ” ê° rank variableì— ëŒ€í•œ í‘œì¤€í¸ì°¨ì´ë‹¤.<br>
{yi^}ì™€ {yi}ì—ì„œ ê°ê° ì§‘í•© ì•ˆì— ì¤‘ë³µë˜ëŠ” ê°’ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ (6)ë²ˆ ì‹ì€ (7)ë²ˆ ì‹ì²˜ëŸ¼ ë‹¤ì‹œ ì“¸ ìˆ˜ ìˆë‹¤. (N: {yi^} ë˜ëŠ” {yi} ì§‘í•©ì˜ í¬ê¸° = ì´ ë°ì´í„° ìˆ˜)

$$
r_r = 1 - {6\sum_i (\Rank(\widehat(y_i))-\Rank(y_i))^2}/{N(N^2-1)}
$$

ì´ ì‹ì— ëŒ€í•œ ì¦ëª…ì€ [ì—¬ê¸°](https://stats.stackexchange.com/questions/89121/prove-the-equivalence-of-the-following-two-formulas-for-spearman-correlation/89211#89211)ë¥¼ ì°¸ê³ í•˜ë©´ ëœë‹¤.

ì°¸ê³ ë¡œ ë§Œì•½ {yi^}ë‚˜ {yi}ì— ì¤‘ë³µ ê°’ì´ ìˆë‹¤ë©´ rank operatorëŠ” fractional ranking operatorê°€ ëœë‹¤.

cf. [fractional ranking](https://en.wikipedia.org/wiki/Ranking#Fractional_ranking_(%221_2.5_2.5_4%22_ranking))

```
ë°ì´í„°ì˜ ì¤‘ë³µ íšŸìˆ˜ê¹Œì§€ ê³ ë ¤í•´ì„œ ìˆœìœ„ë¥¼ ë§¤ê¸°ëŠ” ë°©ì‹ì´ë‹¤.
ì˜ˆë¥¼ ë“¤ì–´ {1.0, 1.0, 2.0, 3.0, 3.0, 4.0, 5.0, 5.0, 5.0}ì´ë¼ëŠ” ë°ì´í„° ì§‘í•©ì´ ìˆë‹¤ê³  í•˜ì.
ê·¸ëƒ¥ ìˆœì„œëŒ€ë¡œ ìˆœìœ„ë¥¼ ë§¤ê¸°ë©´ 1~9ê¹Œì§€ê°€ ë  í…ë°,
- 1, 2ë²ˆ ì¤‘ë³µ (2ê°œ): (1+2)/2=1.5
- 4, 5ë²ˆ ì¤‘ë³µ (2ê°œ): (4+5)/2=4.5
- 7, 8, 9ë²ˆ ì¤‘ë³µ (3ê°œ): (7+8+9)/3=8.0
ìœ„ì™€ ê°™ì€ ì—°ì‚°ì„ ê±°ì³ ìˆœìœ„ ë³€ìˆ˜ ì§‘í•©ì´ {1.5, 1.5, 3.0, 4.5, 4.5, 6.0, 8.0, 8.0, 8.0}ê°€ ëœë‹¤.
```

ì—¬ê¸°ì„œ ranking lossë¥¼ ì¤‘ë³µ ê°’ì´ ì—†ëŠ” ë‘ ë°ì´í„° ì§‘í•© ê°„ì˜ Spearman ìƒê´€ê³„ìˆ˜ë¥¼ ê°€ì§€ê³  ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$$
L_r = \sum_i (\Rank(\widehat(y_i))-\Rank(y_i))^2     (8)
$$

{yi^}ì™€ {yi} ì§‘í•© ë‘˜ ë‹¤ ì¤‘ë³µ ê°’ì´ ì—†ìœ¼ë©´ (7)ë²ˆ ì‹ì„ ë”°ë¥¸ë‹¤ê³  í–ˆëŠ”ë°, ê·¸ë ‡ê²Œ ë˜ë©´ Lrì´ 1-rr(1ì—ì„œ Spearman ìƒê´€ê³„ìˆ˜ë¥¼ ëº€ ê°’)ì— ë¹„ë¡€í•œë‹¤ëŠ” ì ì— ì£¼ëª©í•˜ì.

(âˆµ (8)ë²ˆ ì‹ì„ (7)ë²ˆ ì‹ì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë¨.)

$$
r_r = 1 - 6*L_r / N(N^2 - 1)
L_r = \alpha*(1-r_r), where \alpha = N(N^2-1)/6
$$

ë‹¤ì‹œ ë§í•´ì„œ Lrì´ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ Spearman ìƒê´€ê³„ìˆ˜ëŠ” 1ì— ê°€ê¹Œì›Œì§„ë‹¤ëŠ” ì˜ë¯¸ì´ê³ , ì´ëŠ” ê³§ Lr ê°’ì´ ì‘ì„ìˆ˜ë¡ ì˜ˆì¸¡ê°’ì´ ì •ë‹µê°’ì— ê·¼ì‚¬í•˜ë‹¤ëŠ” ê²ƒê³¼ ê°™ë‹¤.<br>
ë”°ë¼ì„œ Lr ê°’ì„ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµ ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ” ê²ƒì´ ì´ ë…¼ë¬¸ì˜ ì£¼ ëª©ì ì´ë‹¤.

í•œí¸ ì´ë ‡ê²Œ Spearman ìƒê´€ê³„ìˆ˜ì— ê¸°ë°˜í•œ ranking lossëŠ” gradient descent ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì— ì¼ë°˜ì ìœ¼ë¡œ ì˜ ì“°ì´ëŠ” ê¸°ë²•ì€ ì•„ë‹ˆë‹¤. gradientë¼ëŠ” ê±´ ê¸°ìš¸ê¸°ë¥¼ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì— ë¯¸ë¶„ ê°œë…ì´ ë“¤ì–´ê°€ê²Œ ë˜ëŠ”ë° ranking operatorëŠ” ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ê·¸ëŸ°ë° "SoDeep: A Sorting Deep Net to Learn Ranking Loss Surrogates" [2] ë…¼ë¬¸ì—ì„œ rank variableì„ ë¯¸ë¶„í•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì‹ì„ ì¡°ì‘í•´ì„œ Spearman ìƒê´€ê³„ìˆ˜ì— ê¸°ë°˜í•œ ranking lossë¥¼ PyTorchë¡œ êµ¬í˜„í•œ ê²Œ ìˆë‹¤. ì¹œì ˆí•˜ê²Œ [GitHubì— ì½”ë“œë„ ì˜¬ë¼ì™€ ìˆë‹¤](https://github.com/technicolor-research/sodeep).

ë‚˜ëŠ” ì´ê±¸ ë‚´ ì—°êµ¬ì— ì ìš©í•´ ë³¼ ê±°ë¼ì„œ [2]ë¥¼ ì½ìœ¼ë©° ì½”ë“œë¥¼ ì œëŒ€ë¡œ ì´í•´í•´ ë³´ê¸°ë¡œ í–ˆë‹¤.

```python
class SpearmanLoss(torch.nn.Module):
    """ Loss function  inspired by spearmann correlation.self
    Required the trained model to have a good initlization.
    Set lbd to 1 for a few epoch to help with the initialization.
    """
    def __init__(self, sorter_type, seq_len=None, sorter_state_dict=None, lbd=0):
        super(SpearmanLoss, self).__init__()
        self.sorter = model_loader(sorter_type, seq_len, sorter_state_dict)

        self.criterion_mse = torch.nn.MSELoss()
        self.criterionl1 = torch.nn.L1Loss()

        self.lbd = lbd

    def forward(self, mem_pred, mem_gt, pr=False):
        rank_gt = get_rank(mem_gt)

        rank_pred = self.sorter(mem_pred.unsqueeze(0)).view(-1)

        return self.criterion_mse(rank_pred, rank_gt) + self.lbd * self.criterionl1(mem_pred, mem_gt)
```

ë¨¼ì € ì´ê±´ Spearman Lossë¥¼ PyTorch Moduleë¡œ êµ¬í˜„í•œ í´ë˜ìŠ¤ì´ë‹¤.

`__init__` ë©”ì„œë“œì—ì„œ ë³´ë©´ `model_loader` í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ sorterë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë°, ì´ sorterê°€ rank variableë¥¼ ë¯¸ë¶„ ê°€ëŠ¥í•œ ë³€ìˆ˜ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” rank functionì˜ surrogateì´ë‹¤.

![figure](/assets/images/220930/model_loader.png)<br>
Training a differentiable sorter [2].<br>
Î˜B: learnable parameters of a DNN<br>
r: rank from true rank vector rk(y)<br>
r^: rank predicted from the DNN

ìœ„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ ì´í•´ê°€ ì‰½ë‹¤.<br>
dê°œì˜ ë°ì´í„°ë¥¼ ê°€ì§„ yë¼ëŠ” ë²¡í„°ê°€ ìˆê³  yì˜ ëª¨ë“  ì›ì†Œì— ëŒ€í•´ 1ë¶€í„° dê¹Œì§€ rankë¥¼ ë§¤ê¸´ ë²¡í„°ë¥¼ rk(y)ë¼ê³  í•  ë•Œ, ì´ rk(y)ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ sorter ëª¨ë¸ fÎ˜Bë¥¼ ë§Œë“ ë‹¤.<br>
ì¦‰ fÎ˜BëŠ” ranking operatorì˜ ì—­í• ì„ í•˜ëŠ” ëª¨ë¸ì´ê³  ì´ë¥¼ L1 lossë¥¼ ê°€ì§€ê³  í•™ìŠµì‹œí‚´ìœ¼ë¡œì¨ gradient descentë¥¼ ìµœì í™”í•˜ê³ ì í•˜ëŠ” ê²ƒì´ë‹¤.<br>

$$
\min_{\Theta_B} \sum_{n=1}^N \Vert \rk(y_(n)) - f_\Theta_B (y_(n)) \Vert_1  (N: \sample \size)
$$

sorter ë„¤íŠ¸ì›Œí¬ëŠ” ê·¸ëƒ¥ ë‹¨ìˆœí•œ convolutional architectureë¥¼ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë° ì´ ë¶€ë¶„ì— ëŒ€í•œ ì„¤ëª…ì€ ê±´ë„ˆë›°ê² ë‹¤.

ë‹¤ì‹œ ì½”ë“œë¡œ ëŒì•„ì™€ì„œ ì½”ë“œì—ì„œ sorterëŠ” ì´ë¯¸ label ë°ì´í„°ì— ëŒ€í•œ rankë¥¼ ì˜ ë§¤ê¸¸ ìˆ˜ ìˆë‹¤ëŠ” ê°€ì •ì´ ë˜ì–´ìˆê¸° ë•Œë¬¸ì— pretrained modelì´ì–´ì•¼ í•œë‹¤. ë‹¤í–‰íˆ pretrained model íŒŒì¼ë„ í•¨ê»˜ ì œê³µë˜ê³  ìˆì–´ì„œ ì´ê±¸ ì¨ë³´ë©´ ë  ê²ƒ ê°™ë‹¤.<br>
forward ë©”ì„œë“œì—ì„œ ë³´ë©´ get_rankë¼ëŠ” ë©”ì„œë“œê°€ ìˆëŠ”ë° ì´ê±´ ì´ë ‡ê²Œ ìƒê²¼ë‹¤:

```python
def get_rank(batch_score, dim=0):
    rank = torch.argsort(batch_score, dim=dim)
    rank = torch.argsort(rank, dim=dim)
    rank = (rank * -1) + batch_score.size(dim)
    rank = rank.float()
    rank = rank / batch_score.size(dim)

    return rank
```

[2]ì—ì„œ êµ¬í˜„í•œ rank operatorëŠ” ìœ„ì™€ ê°™ì€ë°, [1]ì—ì„œ (ë‚´ê°€ ì´í•´í•œ ê²Œ ë§ë‹¤ë©´) ì¤‘ë³µ ê°’ì´ ìˆì„ ê²ƒê¹Œì§€ ê³ ë ¤í•´ì„œ êµ¬í˜„í•œ get_tiedrank ë©”ì„œë“œë„ ìˆë‹¤.

```python
def get_tiedrank(batch_score, dim=0):
    batch_score = batch_score.cpu()
    rank = stats.rankdata(batch_score)
    rank = stats.rankdata(rank) - 1    
    rank = (rank * -1) + batch_score.size(dim)
    rank = torch.from_numpy(rank).cuda()
    rank = rank.float()
    rank = rank / batch_score.size(dim)  
    return rank
```

get_rankë‚˜ get_tiedrankì˜ ë§¤ê°œë³€ìˆ˜ë¡œëŠ” ground truthê°€ ë“¤ì–´ê°€ë©° return ê°’ì€ ground truthì˜ rankì´ë‹¤.

SpearmanLossë¥¼ loss functionìœ¼ë¡œ ì“°ê²Œ ë˜ë©´ ì´ì™€ ê°™ì´ rank operatorë¥¼ ì´ìš©í•œ ground truthì˜ rankì¸ Rank(yi)ì™€ sorterë¥¼ ì´ìš©í•œ predicted rank Rank(yi^) ê°„ì˜ lossë¥¼ ì—°ì‚°í•˜ëŠ” ê²ƒì´ë‹¤.

---

[1] Cheng, Jian, et al. "Brain age estimation from MRI using cascade networks with ranking loss." IEEE Transactions on Medical Imaging 40.12 (2021): 3400-3412.<br>
[2] Engilberge, Martin, et al. "Sodeep: a sorting deep net to learn ranking loss surrogates." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.<br>
