---
title: "[논문리뷰] RankSim for Deep Imbalanced Regression (2022)"
categories: paper-review
tags: machine-learning python pytorch
use_math: true
---

# RankSim 논문 & 코드 리뷰

[RankSim: Ranking Similarity Regularization for Deep Imbalanced Regression](https://arxiv.org/abs/2205.15236), [Github](https://github.com/BorealisAI/ranksim-imbalanced-regression.git)

## My Preview

### Regularization과 Imbalanced Regression

Regularization이란 weight 값이 너무 커지지 않도록 제한하는 일종의 penalty 조건이다.

한편 regression task가 classification task와 다른 점은 학습 모델이 예측해야 하는 값의 범위가 연속적이라는 것이다.
그래서 imbalanced classification을 위한 method를 regression에 그대로 적용하는 데에는 어려움이 따른다.

전에 정리한 적 있는 [Delving into Deep Imbalanced Regression](https://hei-jung.github.io/paper-review/deep-imbalanced-regression/) 논문에서는 regression 데이터의 연속성을 이용해서 데이터 분포에 따라 loss를 re-weighting 하거나 (LDS), feature 벡터를 보정하는 (FDS) 방법을 소개하였다.
그리고 오늘 리뷰하고자 하는 이 논문은 거기서 확장해서 regularization을 통해 imbalanced regression의 성능을 높이는 방법을 제안하고 있다.

## Introduction

<u>LDS, FDS 등 imbalanced regression 관련 선행 연구와의 차별점:</u> 선행 연구에서는  label space에서 가까이 분포하고 있는 데이터의 관계에 집중하고 있다. RankSim은 label space에서 서로 근처에 있는 데이터 뿐만 아니라 멀리 떨어져 있는 데이터 간의 관계까지 고려하고 있다.

즉 RankSim의 목적은 **label space에서 비슷하면 feature space에서도 비슷하라**는 것이다.

예를 들어..
age estimation example에서 $ z^n $: label space에서 n살에 해당하는 learned feature, $ \sigma(⋅,⋅) $: 두 벡터 간 similarity function (예: cosine 유사도) 라고 할 때, 1, 21, 25, 70살에 대한 이미지 데이터를 가지고 있다고 하자.
그럼 $ z^{21} $에 대해서는 $ \sigma(z^{21}, z^{25}) > \sigma(z^{21}, z^{1}) > \sigma(z^{21}, z^{70}) $를 만족하도록 bias를 줘야 한다.

이걸 그림으로 나타낸 게 Figure 1과 같다.

![Figure1](/assets/images/221114/Figure1.png)
